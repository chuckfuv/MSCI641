{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2849,
     "status": "ok",
     "timestamp": 1563882444445,
     "user": {
      "displayName": "chuck fu",
      "photoUrl": "",
      "userId": "01740202220584032426"
     },
     "user_tz": 240
    },
    "id": "QlPPBgw_7CWT",
    "outputId": "6525ab02-6c89-4e13-c295-ae50fd37a83d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  projectRE.zip\n",
      "  inflating: feature_engineering.py  \n",
      "  inflating: LICENSE                 \n",
      "  inflating: requirements.txt        \n",
      "  inflating: fnc_kfold.py            \n",
      "   creating: __MACOSX/\n",
      "  inflating: __MACOSX/._fnc_kfold.py  \n",
      "  inflating: embedding_matrixRE.npy  \n",
      "   creating: features/\n",
      "  inflating: features/hand.competition.npy  \n",
      "  inflating: features/hand.9.npy     \n",
      "  inflating: features/hand.8.npy     \n",
      " extracting: features/dummy.txt      \n",
      "  inflating: features/overlap.competition.npy  \n",
      "  inflating: features/overlap.9.npy  \n",
      "  inflating: features/refuting.9.npy  \n",
      "  inflating: features/refuting.8.npy  \n",
      "  inflating: features/overlap.8.npy  \n",
      "  inflating: features/polarity.7.npy  \n",
      "  inflating: features/polarity.6.npy  \n",
      "  inflating: features/polarity.competition.npy  \n",
      "  inflating: features/polarity.4.npy  \n",
      "  inflating: features/polarity.5.npy  \n",
      "  inflating: features/polarity.1.npy  \n",
      "  inflating: features/polarity.0.npy  \n",
      "  inflating: features/polarity.2.npy  \n",
      "  inflating: features/polarity.3.npy  \n",
      "  inflating: features/refuting.competition.npy  \n",
      "  inflating: features/polarity.8.npy  \n",
      "  inflating: features/polarity.9.npy  \n",
      "  inflating: features/overlap.holdout.npy  \n",
      "  inflating: features/overlap.5.npy  \n",
      "  inflating: features/refuting.5.npy  \n",
      "  inflating: features/polarity.holdout.npy  \n",
      "  inflating: features/hand.0.npy     \n",
      "  inflating: features/hand.1.npy     \n",
      "  inflating: features/refuting.4.npy  \n",
      "  inflating: features/hand.holdout.npy  \n",
      "  inflating: features/overlap.4.npy  \n",
      "  inflating: features/overlap.6.npy  \n",
      "  inflating: features/refuting.6.npy  \n",
      "  inflating: features/hand.3.npy     \n",
      "  inflating: features/hand.2.npy     \n",
      "  inflating: features/refuting.holdout.npy  \n",
      "  inflating: features/refuting.7.npy  \n",
      "  inflating: features/overlap.7.npy  \n",
      "  inflating: features/overlap.3.npy  \n",
      "  inflating: features/refuting.3.npy  \n",
      "  inflating: features/hand.6.npy     \n",
      "  inflating: features/hand.7.npy     \n",
      "  inflating: features/refuting.2.npy  \n",
      "  inflating: features/overlap.2.npy  \n",
      "  inflating: features/overlap.0.npy  \n",
      "  inflating: features/refuting.0.npy  \n",
      "  inflating: features/hand.5.npy     \n",
      "  inflating: features/hand.4.npy     \n",
      "  inflating: features/refuting.1.npy  \n",
      "  inflating: features/overlap.1.npy  \n",
      " extracting: scores                  \n",
      "   creating: utils/\n",
      "  inflating: utils/generate_test_splits.py  \n",
      "   creating: __MACOSX/utils/\n",
      "  inflating: __MACOSX/utils/._generate_test_splits.py  \n",
      "  inflating: utils/system.py         \n",
      "  inflating: __MACOSX/utils/._system.py  \n",
      " extracting: utils/__init__.py       \n",
      "   creating: utils/__pycache__/\n",
      "  inflating: utils/__pycache__/dataset.cpython-36.pyc  \n",
      "  inflating: utils/__pycache__/generate_test_splits.cpython-36.pyc  \n",
      "  inflating: utils/__pycache__/score.cpython-36.pyc  \n",
      "  inflating: utils/__pycache__/system.cpython-36.pyc  \n",
      "  inflating: utils/__pycache__/__init__.cpython-36.pyc  \n",
      "  inflating: utils/dataset.py        \n",
      "  inflating: __MACOSX/utils/._dataset.py  \n",
      "  inflating: utils/score.py          \n",
      "  inflating: __MACOSX/utils/._score.py  \n",
      "   creating: __pycache__/\n",
      "  inflating: __pycache__/feature_engineering.cpython-36.pyc  \n",
      "   creating: fnc-1/\n",
      "  inflating: fnc-1/competition_test_stances_unlabeled.csv  \n",
      "  inflating: fnc-1/test_bodies.csv   \n",
      "  inflating: fnc-1/competition_test_stances.csv  \n",
      "   creating: __MACOSX/fnc-1/\n",
      "  inflating: __MACOSX/fnc-1/._competition_test_stances.csv  \n",
      "  inflating: fnc-1/train_bodies.csv  \n",
      "  inflating: fnc-1/README.md         \n",
      "  inflating: fnc-1/test_stances_unlabeled.csv  \n",
      "  inflating: __MACOSX/fnc-1/._test_stances_unlabeled.csv  \n",
      "  inflating: fnc-1/competition_test_bodies.csv  \n",
      "  inflating: __MACOSX/fnc-1/._competition_test_bodies.csv  \n",
      "  inflating: fnc-1/train_stances.random.csv  \n",
      "  inflating: fnc-1/train_stances.csv  \n",
      "  inflating: fnc-1/scorer.py         \n",
      "  inflating: README.md               \n",
      "  inflating: baseline.ipynb          \n",
      "  inflating: dummy.npy               \n",
      "   creating: splits/\n",
      " extracting: splits/dummy.txt        \n",
      "  inflating: splits/training_ids.txt  \n",
      "  inflating: splits/hold_out_ids.txt  \n"
     ]
    }
   ],
   "source": [
    "! unzip projectRE.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 189
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 69665,
     "status": "ok",
     "timestamp": 1563882268091,
     "user": {
      "displayName": "chuck fu",
      "photoUrl": "",
      "userId": "01740202220584032426"
     },
     "user_tz": 240
    },
    "id": "aJhSrLhn7kGM",
    "outputId": "41e69ac2-857b-4c50-980a-14605bdf782c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ezAC_GvJ7oWi"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "#from gensim.models import Word2Vec\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout, BatchNormalization, Activation, Bidirectional, Flatten\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import callbacks, regularizers\n",
    "from utils.dataset import DataSet\n",
    "from utils.generate_test_splits import train_vali_split, get_stances_for_folds\n",
    "from utils.score import report_score, LABELS, score_submission\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from utils.system import parse_params, check_version\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J4V0Z9HJ7qjl"
   },
   "outputs": [],
   "source": [
    "train_stance = pd.read_csv('train_stances.csv')\n",
    "train_body = pd.read_csv('train_bodies.csv')\n",
    "compe_stance = pd.read_csv('competition_test_stances.csv')\n",
    "compe_body = pd.read_csv('competition_test_bodies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hLWySzIV749A"
   },
   "outputs": [],
   "source": [
    "train = train_stance.merge(train_body, how = 'left', on = 'Body ID')\n",
    "compe = compe_stance.merge(compe_body,how = 'left', on = 'Body ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 101609,
     "status": "ok",
     "timestamp": 1563890521818,
     "user": {
      "displayName": "chuck fu",
      "photoUrl": "",
      "userId": "01740202220584032426"
     },
     "user_tz": 240
    },
    "id": "DWxsLsv39ROG",
    "outputId": "da8b52c1-09cf-459f-f155-2624efa8feea"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Headline</th>\n",
       "      <th>Body ID</th>\n",
       "      <th>Stance</th>\n",
       "      <th>articleBody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Police find mass graves with at least '15 bodi...</td>\n",
       "      <td>712</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>Danny Boyle is directing the untitled film\\n\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hundreds of Palestinians flee floods in Gaza a...</td>\n",
       "      <td>158</td>\n",
       "      <td>agree</td>\n",
       "      <td>Hundreds of Palestinians were evacuated from t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Christian Bale passes on role of Steve Jobs, a...</td>\n",
       "      <td>137</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>30-year-old Moscow resident was hospitalized w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HBO and Apple in Talks for $15/Month Apple TV ...</td>\n",
       "      <td>1034</td>\n",
       "      <td>unrelated</td>\n",
       "      <td>(Reuters) - A Canadian soldier was shot at the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Spider burrowed through tourist's stomach and ...</td>\n",
       "      <td>1923</td>\n",
       "      <td>disagree</td>\n",
       "      <td>Fear not arachnophobes, the story of Bunbury's...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Headline  ...                                        articleBody\n",
       "0  Police find mass graves with at least '15 bodi...  ...  Danny Boyle is directing the untitled film\\n\\n...\n",
       "1  Hundreds of Palestinians flee floods in Gaza a...  ...  Hundreds of Palestinians were evacuated from t...\n",
       "2  Christian Bale passes on role of Steve Jobs, a...  ...  30-year-old Moscow resident was hospitalized w...\n",
       "3  HBO and Apple in Talks for $15/Month Apple TV ...  ...  (Reuters) - A Canadian soldier was shot at the...\n",
       "4  Spider burrowed through tourist's stomach and ...  ...  Fear not arachnophobes, the story of Bunbury's...\n",
       "\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CI5yTUR69HcJ"
   },
   "outputs": [],
   "source": [
    "def stance_to_onehot(stance):\n",
    "    li = []\n",
    "    LABELS = ['agree', 'disagree', 'discuss', 'unrelated']\n",
    "    for i in stance:\n",
    "        if i == LABELS[0]:\n",
    "            li.append([1,0,0,0])\n",
    "        elif i == LABELS[1]:\n",
    "            li.append([0,1,0,0])\n",
    "        elif i == LABELS[2]:\n",
    "            li.append([0,0,1,0])\n",
    "        elif i == LABELS[3]:\n",
    "            li.append([0,0,0,1])\n",
    "    return np.array(li)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l-tYBmKC9IJf"
   },
   "outputs": [],
   "source": [
    "token_pattern = r\"(?u)\\b\\w\\w+\\b\"\n",
    "def get_tokens(line,\n",
    "                    token_pattern=token_pattern):\n",
    "    token_pattern = re.compile(token_pattern, flags = re.UNICODE)\n",
    "    tokens = [x.lower() for x in token_pattern.findall(line)]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KziYzOUFlQRn"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_vali, y_train, y_vali = train_test_split(train, train['Stance'], random_state=10,shuffle =False,test_size =0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gvaAKkJg-dOd"
   },
   "outputs": [],
   "source": [
    "train_headline = [get_tokens(text) for text in X_train['Headline'].tolist()]\n",
    "train_body = [get_tokens(text) for text in X_train['articleBody'].tolist()]\n",
    "vali_headline = [get_tokens(text) for text in X_vali['Headline'].tolist()]\n",
    "vali_body = [get_tokens(text) for text in X_vali['articleBody'].tolist()]\n",
    "compe_headline = [get_tokens(text) for text in compe['Headline'].tolist()]\n",
    "compe_body = [get_tokens(text) for text in compe['articleBody'].tolist()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 19803,
     "status": "ok",
     "timestamp": 1563898259993,
     "user": {
      "displayName": "chuck fu",
      "photoUrl": "",
      "userId": "01740202220584032426"
     },
     "user_tz": 240
    },
    "id": "Zk6KX-mu-myY",
    "outputId": "9263595f-b805-4943-fc7e-510cde79e6fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29419"
      ]
     },
     "execution_count": 136,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "corpus = train_headline+train_body+compe_headline+compe_body+vali_headline+vali_body\n",
    "tokenizer.fit_on_texts([' '.join(seq) for seq in corpus])\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n6cyXxk0uOAQ"
   },
   "outputs": [],
   "source": [
    "train_headline_seq = tokenizer.texts_to_sequences(train_headline)\n",
    "train_body_seq = tokenizer.texts_to_sequences(train_body)\n",
    "vali_headline_seq = tokenizer.texts_to_sequences(vali_headline)\n",
    "vali_body_seq = tokenizer.texts_to_sequences(vali_body)\n",
    "\n",
    "competition_headline_seq = tokenizer.texts_to_sequences(compe_headline)\n",
    "competition_body_seq = tokenizer.texts_to_sequences(compe_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12135,
     "status": "ok",
     "timestamp": 1563898283034,
     "user": {
      "displayName": "chuck fu",
      "photoUrl": "",
      "userId": "01740202220584032426"
     },
     "user_tz": 240
    },
    "id": "gNG9fRgmucBM",
    "outputId": "50612ebf-9087-4aeb-97b0-8e47a1174831"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10th Percentile body Length: 99\n"
     ]
    }
   ],
   "source": [
    "MAX_BODY_LENGTH = int(np.percentile([len(doc) for doc in train_body_seq+competition_body_seq],10))\n",
    "\n",
    "print('10th Percentile body Length:', MAX_BODY_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10130,
     "status": "ok",
     "timestamp": 1563898283035,
     "user": {
      "displayName": "chuck fu",
      "photoUrl": "",
      "userId": "01740202220584032426"
     },
     "user_tz": 240
    },
    "id": "5wQ-pKN6v-rT",
    "outputId": "95e5bd8b-7853-44b8-fc17-92555ce03c1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90th Percentile headline Length: 16\n",
      "90th Percentile body Length: 353\n"
     ]
    }
   ],
   "source": [
    "MAX_HEADLINE_LENGTH = int(np.percentile([len(doc) for doc in train_headline_seq+competition_headline_seq], 90))\n",
    "\n",
    "print('90th Percentile headline Length:', MAX_HEADLINE_LENGTH)\n",
    "MAX_HEADLINE_LENGTH =15\n",
    "MAX_BODY_LENGTH = int(np.mean([len(doc) for doc in train_body_seq+competition_body_seq]))\n",
    "\n",
    "print('90th Percentile body Length:', MAX_BODY_LENGTH)\n",
    "MAX_BODY_TRUNC = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hNkloop9xl-l"
   },
   "outputs": [],
   "source": [
    "train_headline = pad_sequences(train_headline_seq, maxlen=MAX_HEADLINE_LENGTH, padding='post', truncating='post')\n",
    "train_first_body = pad_sequences(train_body_seq, maxlen=MAX_BODY_TRUNC, padding='post', truncating='post')\n",
    "train_last_body = pad_sequences(train_body_seq, maxlen=MAX_BODY_TRUNC, padding='post', truncating='pre')\n",
    "\n",
    "vali_headline = pad_sequences(vali_headline_seq, maxlen=MAX_HEADLINE_LENGTH, padding='post', truncating='post')\n",
    "vali_first_body = pad_sequences(vali_body_seq, maxlen=MAX_BODY_TRUNC, padding='post', truncating='post')\n",
    "vali_last_body = pad_sequences(vali_body_seq, maxlen=MAX_BODY_TRUNC, padding='post', truncating='pre')\n",
    "\n",
    "competition_headline = pad_sequences(competition_headline_seq, maxlen=MAX_HEADLINE_LENGTH, padding='post', truncating='post')\n",
    "competition_first_body = pad_sequences(competition_body_seq, maxlen=MAX_BODY_TRUNC, padding='post', truncating='post')\n",
    "competition_last_body = pad_sequences(competition_body_seq, maxlen=MAX_BODY_TRUNC, padding='post', truncating='pre')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 162113,
     "status": "ok",
     "timestamp": 1563898647996,
     "user": {
      "displayName": "chuck fu",
      "photoUrl": "",
      "userId": "01740202220584032426"
     },
     "user_tz": 240
    },
    "id": "5KPgia3hnJxK",
    "outputId": "6a30c892-d604-4082-c64f-85d1b7c4f9f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1917494 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# load the whole embedding into memory\n",
    "embeddings_index = dict()\n",
    "f = open('glove.42B.300d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VsidkwBnzlO5"
   },
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = np.zeros((vocab_size, 300))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "del embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D9jn54XlyZt0"
   },
   "outputs": [],
   "source": [
    "def builtWeightSharingModel(drop_out_rate=0.5,dense_neuron = 32):\n",
    "    headline_input = Input(shape=(MAX_HEADLINE_LENGTH,), name='headline_input')\n",
    "    first_body_input = Input(shape=(MAX_BODY_TRUNC,), name='first_body_input')\n",
    "    last_body_input = Input(shape=(MAX_BODY_TRUNC,), name='last_body_input')\n",
    "    e = Embedding(input_dim=len(tokenizer.word_index)+1,\n",
    "                          output_dim=EMBEDDING_DIM,\n",
    "                          weights = [embedding_matrix], trainable=False, name='headline_word_embedding_layer', \n",
    "                          mask_zero=True)\n",
    "    headline_embedding = e(headline_input)\n",
    "    first_body_embedding = e(first_body_input)\n",
    "    last_body_embedding = e(last_body_input)\n",
    "    encoder_outputs, state_h, state_c = LSTM(LSTM_DIM, return_sequences=False, return_state = True,dropout=0.5,recurrent_dropout = 0.5, name='lstm_layer_encoder')(headline_embedding)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    decoder = LSTM(LSTM_DIM, return_sequences=False, return_state = True,dropout=0.5,recurrent_dropout = 0.5, name='lstm_layer_first_body')\n",
    "    first_body_outputs, _, _ = decoder(first_body_embedding, initial_state=encoder_states)\n",
    "    last_body_outputs, _, _ = decoder(last_body_embedding, initial_state=encoder_states)\n",
    "    x = tf.keras.layers.concatenate([first_body_outputs, last_body_outputs])\n",
    "    x = Dropout(rate=drop_out_rate, name='dropout_0')(x)\n",
    "    #x = Dense(dense_neuron, activation='relu')(x)\n",
    "    #x = Dropout(rate=drop_out_rate, name='dropout_1')(x)\n",
    "    #x = Dense(dense_neuron, activation='relu')(x)\n",
    "    #x = Dropout(rate=drop_out_rate, name='dropout_2')(x)\n",
    "\n",
    "    # And finally we add the main logistic regression layer\n",
    "    main_output = Dense(4, activation='softmax', name='main_output')(x)\n",
    "    model = Model(inputs=[headline_input, first_body_input, last_body_input], outputs=main_output)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SHu8YjoRyqGV"
   },
   "outputs": [],
   "source": [
    "train_stance_onehot = stance_to_onehot(y_train)\n",
    "vali_stance_onehot = stance_to_onehot(y_vali)\n",
    "competition_stance_onehot = stance_to_onehot(compe['Stance'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 697,
     "status": "ok",
     "timestamp": 1563887157840,
     "user": {
      "displayName": "chuck fu",
      "photoUrl": "",
      "userId": "01740202220584032426"
     },
     "user_tz": 240
    },
    "id": "m9EwJ81n7__I",
    "outputId": "018e7ceb-c7bd-473e-870c-f51b4462af62"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49972"
      ]
     },
     "execution_count": 98,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(competition_stance_onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cM9QeMzyyc9v"
   },
   "outputs": [],
   "source": [
    "def trainModel(model,train_input,vali_input, batch=1000,ep=200):\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    output_directory = ''\n",
    "    model_checkpoint = callbacks.ModelCheckpoint(os.path.join(output_directory , 'weights.{epoch:02d}-{val_loss:.2f}.hdf5'),save_best_only=True,mode='auto',period=10)\n",
    "    '''\n",
    "    model.fit([np.concatenate((train_headline, hold_out_headline), axis=0), np.concatenate((train_first_body, hold_out_first_body), axis=0), np.concatenate((train_last_body, hold_out_last_body), axis=0)], np.concatenate((train_stance_onehot, hold_out_stance_onehot), axis=0),\n",
    "          batch_size=batch,\n",
    "          epochs=ep,\n",
    "          validation_data=([competition_headline, competition_first_body, competition_last_body], competition_stance_onehot),callbacks=[model_checkpoint])\n",
    "    '''\n",
    "    model.fit(train_input, train_stance_onehot,\n",
    "          batch_size=batch,\n",
    "          epochs=ep,\n",
    "          validation_data=(vali_input, vali_stance_onehot),callbacks=[model_checkpoint])\n",
    "    #'''\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "POammppjzVuF"
   },
   "outputs": [],
   "source": [
    "LSTM_DIM = 300\n",
    "EMBEDDING_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZY512Lu6aOZ1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3364999,
     "status": "ok",
     "timestamp": 1563902384900,
     "user": {
      "displayName": "chuck fu",
      "photoUrl": "",
      "userId": "01740202220584032426"
     },
     "user_tz": 240
    },
    "id": "6w-0_Y0WzPwF",
    "outputId": "787ab57b-1ca5-43d8-e313-af90fb62b6c9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 16:23:44.770649 140219664385920 callbacks.py:875] `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "first_body_input (InputLayer)   [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "headline_input (InputLayer)     [(None, 15)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "headline_word_embedding_layer ( multiple             8825700     headline_input[0][0]             \n",
      "                                                                 first_body_input[0][0]           \n",
      "                                                                 last_body_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "last_body_input (InputLayer)    [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_layer_encoder (LSTM)       [(None, 300), (None, 721200      headline_word_embedding_layer[0][\n",
      "__________________________________________________________________________________________________\n",
      "lstm_layer_first_body (LSTM)    [(None, 300), (None, 721200      headline_word_embedding_layer[1][\n",
      "                                                                 lstm_layer_encoder[0][1]         \n",
      "                                                                 lstm_layer_encoder[0][2]         \n",
      "                                                                 headline_word_embedding_layer[2][\n",
      "                                                                 lstm_layer_encoder[0][1]         \n",
      "                                                                 lstm_layer_encoder[0][2]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 600)          0           lstm_layer_first_body[0][0]      \n",
      "                                                                 lstm_layer_first_body[1][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_0 (Dropout)             (None, 600)          0           concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 4)            2404        dropout_0[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 10,270,504\n",
      "Trainable params: 1,444,804\n",
      "Non-trainable params: 8,825,700\n",
      "__________________________________________________________________________________________________\n",
      "Train on 44974 samples, validate on 4998 samples\n",
      "Epoch 1/200\n",
      "44974/44974 [==============================] - 21s 459us/sample - loss: 0.8308 - acc: 0.7189 - val_loss: 0.7572 - val_acc: 0.7321\n",
      "Epoch 2/200\n",
      "44974/44974 [==============================] - 17s 379us/sample - loss: 0.7605 - acc: 0.7333 - val_loss: 0.7056 - val_acc: 0.7533\n",
      "Epoch 3/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.7222 - acc: 0.7445 - val_loss: 0.6678 - val_acc: 0.7693\n",
      "Epoch 4/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.6892 - acc: 0.7558 - val_loss: 0.6317 - val_acc: 0.7835\n",
      "Epoch 5/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.6594 - acc: 0.7644 - val_loss: 0.5840 - val_acc: 0.7867\n",
      "Epoch 6/200\n",
      "44974/44974 [==============================] - 17s 376us/sample - loss: 0.6144 - acc: 0.7754 - val_loss: 0.5366 - val_acc: 0.8039\n",
      "Epoch 7/200\n",
      "44974/44974 [==============================] - 17s 375us/sample - loss: 0.5661 - acc: 0.7859 - val_loss: 0.4846 - val_acc: 0.8159\n",
      "Epoch 8/200\n",
      "44974/44974 [==============================] - 17s 376us/sample - loss: 0.5153 - acc: 0.8017 - val_loss: 0.4361 - val_acc: 0.8265\n",
      "Epoch 9/200\n",
      "44974/44974 [==============================] - 17s 375us/sample - loss: 0.4775 - acc: 0.8143 - val_loss: 0.3962 - val_acc: 0.8379\n",
      "Epoch 10/200\n",
      "44974/44974 [==============================] - 20s 439us/sample - loss: 0.4455 - acc: 0.8254 - val_loss: 0.3835 - val_acc: 0.8493\n",
      "Epoch 11/200\n",
      "44974/44974 [==============================] - 17s 377us/sample - loss: 0.4194 - acc: 0.8369 - val_loss: 0.3431 - val_acc: 0.8675\n",
      "Epoch 12/200\n",
      "44974/44974 [==============================] - 17s 376us/sample - loss: 0.3914 - acc: 0.8479 - val_loss: 0.3208 - val_acc: 0.8727\n",
      "Epoch 13/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.3715 - acc: 0.8590 - val_loss: 0.3014 - val_acc: 0.8818\n",
      "Epoch 14/200\n",
      "44974/44974 [==============================] - 17s 375us/sample - loss: 0.3491 - acc: 0.8667 - val_loss: 0.2916 - val_acc: 0.8864\n",
      "Epoch 15/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.3304 - acc: 0.8735 - val_loss: 0.2775 - val_acc: 0.8906\n",
      "Epoch 16/200\n",
      "44974/44974 [==============================] - 17s 375us/sample - loss: 0.3132 - acc: 0.8815 - val_loss: 0.2456 - val_acc: 0.9082\n",
      "Epoch 17/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.2942 - acc: 0.8882 - val_loss: 0.2302 - val_acc: 0.9092\n",
      "Epoch 18/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.2853 - acc: 0.8912 - val_loss: 0.2236 - val_acc: 0.9134\n",
      "Epoch 19/200\n",
      "44974/44974 [==============================] - 17s 375us/sample - loss: 0.2731 - acc: 0.8955 - val_loss: 0.2157 - val_acc: 0.9184\n",
      "Epoch 20/200\n",
      "44974/44974 [==============================] - 17s 376us/sample - loss: 0.2585 - acc: 0.9023 - val_loss: 0.2077 - val_acc: 0.9226\n",
      "Epoch 21/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.2488 - acc: 0.9063 - val_loss: 0.2014 - val_acc: 0.9242\n",
      "Epoch 22/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.2386 - acc: 0.9088 - val_loss: 0.1902 - val_acc: 0.9294\n",
      "Epoch 23/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.2295 - acc: 0.9134 - val_loss: 0.1780 - val_acc: 0.9322\n",
      "Epoch 24/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.2169 - acc: 0.9188 - val_loss: 0.1731 - val_acc: 0.9382\n",
      "Epoch 25/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.2138 - acc: 0.9195 - val_loss: 0.1759 - val_acc: 0.9382\n",
      "Epoch 26/200\n",
      "44974/44974 [==============================] - 17s 375us/sample - loss: 0.2007 - acc: 0.9250 - val_loss: 0.1552 - val_acc: 0.9434\n",
      "Epoch 27/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.1933 - acc: 0.9273 - val_loss: 0.1589 - val_acc: 0.9434\n",
      "Epoch 28/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.1909 - acc: 0.9287 - val_loss: 0.1470 - val_acc: 0.9480\n",
      "Epoch 29/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.1816 - acc: 0.9332 - val_loss: 0.1445 - val_acc: 0.9478\n",
      "Epoch 30/200\n",
      "44974/44974 [==============================] - 17s 376us/sample - loss: 0.1752 - acc: 0.9341 - val_loss: 0.1518 - val_acc: 0.9490\n",
      "Epoch 31/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.1689 - acc: 0.9361 - val_loss: 0.1352 - val_acc: 0.9500\n",
      "Epoch 32/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.1640 - acc: 0.9374 - val_loss: 0.1294 - val_acc: 0.9548\n",
      "Epoch 33/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.1577 - acc: 0.9411 - val_loss: 0.1325 - val_acc: 0.9540\n",
      "Epoch 34/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.1490 - acc: 0.9443 - val_loss: 0.1306 - val_acc: 0.9522\n",
      "Epoch 35/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.1478 - acc: 0.9433 - val_loss: 0.1336 - val_acc: 0.9530\n",
      "Epoch 36/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.1455 - acc: 0.9451 - val_loss: 0.1224 - val_acc: 0.9572\n",
      "Epoch 37/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.1396 - acc: 0.9472 - val_loss: 0.1180 - val_acc: 0.9600\n",
      "Epoch 38/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.1386 - acc: 0.9481 - val_loss: 0.1183 - val_acc: 0.9588\n",
      "Epoch 39/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.1339 - acc: 0.9505 - val_loss: 0.1159 - val_acc: 0.9608\n",
      "Epoch 40/200\n",
      "44974/44974 [==============================] - 17s 375us/sample - loss: 0.1314 - acc: 0.9503 - val_loss: 0.1125 - val_acc: 0.9624\n",
      "Epoch 41/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.1262 - acc: 0.9530 - val_loss: 0.1140 - val_acc: 0.9606\n",
      "Epoch 42/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.1219 - acc: 0.9544 - val_loss: 0.1098 - val_acc: 0.9624\n",
      "Epoch 43/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.1212 - acc: 0.9549 - val_loss: 0.1084 - val_acc: 0.9628\n",
      "Epoch 44/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.1196 - acc: 0.9556 - val_loss: 0.1101 - val_acc: 0.9640\n",
      "Epoch 45/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.1156 - acc: 0.9562 - val_loss: 0.1084 - val_acc: 0.9632\n",
      "Epoch 46/200\n",
      "44974/44974 [==============================] - 17s 375us/sample - loss: 0.1131 - acc: 0.9567 - val_loss: 0.1019 - val_acc: 0.9658\n",
      "Epoch 47/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.1105 - acc: 0.9584 - val_loss: 0.1024 - val_acc: 0.9648\n",
      "Epoch 48/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.1102 - acc: 0.9585 - val_loss: 0.1033 - val_acc: 0.9652\n",
      "Epoch 49/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.1063 - acc: 0.9591 - val_loss: 0.0991 - val_acc: 0.9666\n",
      "Epoch 50/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.1027 - acc: 0.9611 - val_loss: 0.0998 - val_acc: 0.9678\n",
      "Epoch 51/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.1006 - acc: 0.9624 - val_loss: 0.1013 - val_acc: 0.9660\n",
      "Epoch 52/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.0994 - acc: 0.9624 - val_loss: 0.0977 - val_acc: 0.9678\n",
      "Epoch 53/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0962 - acc: 0.9632 - val_loss: 0.0966 - val_acc: 0.9672\n",
      "Epoch 54/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0949 - acc: 0.9648 - val_loss: 0.0947 - val_acc: 0.9670\n",
      "Epoch 55/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.0950 - acc: 0.9647 - val_loss: 0.0972 - val_acc: 0.9680\n",
      "Epoch 56/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0918 - acc: 0.9663 - val_loss: 0.0937 - val_acc: 0.9682\n",
      "Epoch 57/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0899 - acc: 0.9668 - val_loss: 0.0955 - val_acc: 0.9688\n",
      "Epoch 58/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.0865 - acc: 0.9677 - val_loss: 0.0936 - val_acc: 0.9696\n",
      "Epoch 59/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.0879 - acc: 0.9670 - val_loss: 0.0919 - val_acc: 0.9700\n",
      "Epoch 60/200\n",
      "44974/44974 [==============================] - 17s 375us/sample - loss: 0.0892 - acc: 0.9675 - val_loss: 0.0954 - val_acc: 0.9690\n",
      "Epoch 61/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0870 - acc: 0.9674 - val_loss: 0.0893 - val_acc: 0.9712\n",
      "Epoch 62/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0839 - acc: 0.9683 - val_loss: 0.0906 - val_acc: 0.9710\n",
      "Epoch 63/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0825 - acc: 0.9688 - val_loss: 0.0876 - val_acc: 0.9716\n",
      "Epoch 64/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0783 - acc: 0.9698 - val_loss: 0.0954 - val_acc: 0.9708\n",
      "Epoch 65/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0770 - acc: 0.9713 - val_loss: 0.0903 - val_acc: 0.9722\n",
      "Epoch 66/200\n",
      "44974/44974 [==============================] - 17s 370us/sample - loss: 0.0779 - acc: 0.9703 - val_loss: 0.0881 - val_acc: 0.9724\n",
      "Epoch 67/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0766 - acc: 0.9705 - val_loss: 0.0899 - val_acc: 0.9712\n",
      "Epoch 68/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0741 - acc: 0.9721 - val_loss: 0.0872 - val_acc: 0.9732\n",
      "Epoch 69/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0738 - acc: 0.9724 - val_loss: 0.0869 - val_acc: 0.9736\n",
      "Epoch 70/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0736 - acc: 0.9730 - val_loss: 0.0839 - val_acc: 0.9742\n",
      "Epoch 71/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0741 - acc: 0.9720 - val_loss: 0.0826 - val_acc: 0.9754\n",
      "Epoch 72/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0698 - acc: 0.9735 - val_loss: 0.0898 - val_acc: 0.9730\n",
      "Epoch 73/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0714 - acc: 0.9730 - val_loss: 0.0868 - val_acc: 0.9744\n",
      "Epoch 74/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0689 - acc: 0.9736 - val_loss: 0.0918 - val_acc: 0.9736\n",
      "Epoch 75/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0699 - acc: 0.9733 - val_loss: 0.0856 - val_acc: 0.9744\n",
      "Epoch 76/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0659 - acc: 0.9762 - val_loss: 0.0888 - val_acc: 0.9740\n",
      "Epoch 77/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0673 - acc: 0.9750 - val_loss: 0.0850 - val_acc: 0.9750\n",
      "Epoch 78/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0641 - acc: 0.9766 - val_loss: 0.0837 - val_acc: 0.9744\n",
      "Epoch 79/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0655 - acc: 0.9755 - val_loss: 0.0865 - val_acc: 0.9740\n",
      "Epoch 80/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0669 - acc: 0.9755 - val_loss: 0.0781 - val_acc: 0.9770\n",
      "Epoch 81/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0623 - acc: 0.9769 - val_loss: 0.0864 - val_acc: 0.9758\n",
      "Epoch 82/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0624 - acc: 0.9766 - val_loss: 0.0821 - val_acc: 0.9768\n",
      "Epoch 83/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0612 - acc: 0.9771 - val_loss: 0.0869 - val_acc: 0.9750\n",
      "Epoch 84/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0604 - acc: 0.9776 - val_loss: 0.0860 - val_acc: 0.9768\n",
      "Epoch 85/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0606 - acc: 0.9769 - val_loss: 0.0878 - val_acc: 0.9758\n",
      "Epoch 86/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0610 - acc: 0.9765 - val_loss: 0.0901 - val_acc: 0.9750\n",
      "Epoch 87/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0588 - acc: 0.9780 - val_loss: 0.0895 - val_acc: 0.9748\n",
      "Epoch 88/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0587 - acc: 0.9778 - val_loss: 0.0877 - val_acc: 0.9744\n",
      "Epoch 89/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0598 - acc: 0.9779 - val_loss: 0.0851 - val_acc: 0.9768\n",
      "Epoch 90/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0571 - acc: 0.9797 - val_loss: 0.0809 - val_acc: 0.9762\n",
      "Epoch 91/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0555 - acc: 0.9792 - val_loss: 0.0897 - val_acc: 0.9762\n",
      "Epoch 92/200\n",
      "44974/44974 [==============================] - 17s 370us/sample - loss: 0.0578 - acc: 0.9781 - val_loss: 0.0845 - val_acc: 0.9762\n",
      "Epoch 93/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0549 - acc: 0.9795 - val_loss: 0.0822 - val_acc: 0.9780\n",
      "Epoch 94/200\n",
      "44974/44974 [==============================] - 17s 370us/sample - loss: 0.0549 - acc: 0.9791 - val_loss: 0.0817 - val_acc: 0.9772\n",
      "Epoch 95/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0548 - acc: 0.9799 - val_loss: 0.0823 - val_acc: 0.9780\n",
      "Epoch 96/200\n",
      "44974/44974 [==============================] - 17s 370us/sample - loss: 0.0534 - acc: 0.9805 - val_loss: 0.0847 - val_acc: 0.9778\n",
      "Epoch 97/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0534 - acc: 0.9812 - val_loss: 0.0817 - val_acc: 0.9788\n",
      "Epoch 98/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0509 - acc: 0.9812 - val_loss: 0.0824 - val_acc: 0.9772\n",
      "Epoch 99/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0501 - acc: 0.9812 - val_loss: 0.0871 - val_acc: 0.9762\n",
      "Epoch 100/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0505 - acc: 0.9817 - val_loss: 0.0850 - val_acc: 0.9764\n",
      "Epoch 101/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0501 - acc: 0.9815 - val_loss: 0.0862 - val_acc: 0.9774\n",
      "Epoch 102/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0502 - acc: 0.9815 - val_loss: 0.0827 - val_acc: 0.9774\n",
      "Epoch 103/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0498 - acc: 0.9813 - val_loss: 0.0812 - val_acc: 0.9788\n",
      "Epoch 104/200\n",
      "44974/44974 [==============================] - 17s 370us/sample - loss: 0.0499 - acc: 0.9823 - val_loss: 0.0816 - val_acc: 0.9786\n",
      "Epoch 105/200\n",
      "44974/44974 [==============================] - 17s 370us/sample - loss: 0.0485 - acc: 0.9820 - val_loss: 0.0800 - val_acc: 0.9786\n",
      "Epoch 106/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0491 - acc: 0.9818 - val_loss: 0.0771 - val_acc: 0.9778\n",
      "Epoch 107/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0487 - acc: 0.9821 - val_loss: 0.0814 - val_acc: 0.9780\n",
      "Epoch 108/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0484 - acc: 0.9817 - val_loss: 0.0817 - val_acc: 0.9790\n",
      "Epoch 109/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0475 - acc: 0.9819 - val_loss: 0.0817 - val_acc: 0.9776\n",
      "Epoch 110/200\n",
      "44974/44974 [==============================] - 17s 370us/sample - loss: 0.0446 - acc: 0.9830 - val_loss: 0.0848 - val_acc: 0.9784\n",
      "Epoch 111/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0451 - acc: 0.9832 - val_loss: 0.0835 - val_acc: 0.9772\n",
      "Epoch 112/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0452 - acc: 0.9838 - val_loss: 0.0861 - val_acc: 0.9790\n",
      "Epoch 113/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0462 - acc: 0.9827 - val_loss: 0.0800 - val_acc: 0.9798\n",
      "Epoch 114/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0455 - acc: 0.9839 - val_loss: 0.0801 - val_acc: 0.9796\n",
      "Epoch 115/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0435 - acc: 0.9837 - val_loss: 0.0773 - val_acc: 0.9796\n",
      "Epoch 116/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0438 - acc: 0.9837 - val_loss: 0.0766 - val_acc: 0.9806\n",
      "Epoch 117/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0448 - acc: 0.9841 - val_loss: 0.0829 - val_acc: 0.9790\n",
      "Epoch 118/200\n",
      "44974/44974 [==============================] - 17s 370us/sample - loss: 0.0413 - acc: 0.9846 - val_loss: 0.0841 - val_acc: 0.9796\n",
      "Epoch 119/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0417 - acc: 0.9844 - val_loss: 0.0856 - val_acc: 0.9786\n",
      "Epoch 120/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0424 - acc: 0.9849 - val_loss: 0.0875 - val_acc: 0.9796\n",
      "Epoch 121/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0402 - acc: 0.9854 - val_loss: 0.0856 - val_acc: 0.9804\n",
      "Epoch 122/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0428 - acc: 0.9838 - val_loss: 0.0799 - val_acc: 0.9804\n",
      "Epoch 123/200\n",
      "44974/44974 [==============================] - 17s 370us/sample - loss: 0.0427 - acc: 0.9851 - val_loss: 0.0805 - val_acc: 0.9796\n",
      "Epoch 124/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0394 - acc: 0.9854 - val_loss: 0.0822 - val_acc: 0.9794\n",
      "Epoch 125/200\n",
      "44974/44974 [==============================] - 17s 370us/sample - loss: 0.0412 - acc: 0.9854 - val_loss: 0.0773 - val_acc: 0.9810\n",
      "Epoch 126/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0414 - acc: 0.9848 - val_loss: 0.0763 - val_acc: 0.9822\n",
      "Epoch 127/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0402 - acc: 0.9855 - val_loss: 0.0779 - val_acc: 0.9818\n",
      "Epoch 128/200\n",
      "44974/44974 [==============================] - 17s 370us/sample - loss: 0.0398 - acc: 0.9853 - val_loss: 0.0739 - val_acc: 0.9826\n",
      "Epoch 129/200\n",
      "44974/44974 [==============================] - 17s 370us/sample - loss: 0.0390 - acc: 0.9859 - val_loss: 0.0794 - val_acc: 0.9814\n",
      "Epoch 130/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0354 - acc: 0.9872 - val_loss: 0.0852 - val_acc: 0.9808\n",
      "Epoch 131/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0374 - acc: 0.9873 - val_loss: 0.0788 - val_acc: 0.9828\n",
      "Epoch 132/200\n",
      "44974/44974 [==============================] - 17s 370us/sample - loss: 0.0385 - acc: 0.9857 - val_loss: 0.0798 - val_acc: 0.9828\n",
      "Epoch 133/200\n",
      "44974/44974 [==============================] - 17s 370us/sample - loss: 0.0361 - acc: 0.9871 - val_loss: 0.0748 - val_acc: 0.9824\n",
      "Epoch 134/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0374 - acc: 0.9868 - val_loss: 0.0746 - val_acc: 0.9820\n",
      "Epoch 135/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0342 - acc: 0.9878 - val_loss: 0.0815 - val_acc: 0.9824\n",
      "Epoch 136/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0360 - acc: 0.9869 - val_loss: 0.0834 - val_acc: 0.9800\n",
      "Epoch 137/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0372 - acc: 0.9865 - val_loss: 0.0812 - val_acc: 0.9820\n",
      "Epoch 138/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0357 - acc: 0.9872 - val_loss: 0.0792 - val_acc: 0.9820\n",
      "Epoch 139/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0351 - acc: 0.9873 - val_loss: 0.0689 - val_acc: 0.9842\n",
      "Epoch 140/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0353 - acc: 0.9871 - val_loss: 0.0701 - val_acc: 0.9844\n",
      "Epoch 141/200\n",
      "44974/44974 [==============================] - 17s 370us/sample - loss: 0.0376 - acc: 0.9864 - val_loss: 0.0744 - val_acc: 0.9846\n",
      "Epoch 142/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0331 - acc: 0.9880 - val_loss: 0.0747 - val_acc: 0.9846\n",
      "Epoch 143/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0350 - acc: 0.9874 - val_loss: 0.0788 - val_acc: 0.9826\n",
      "Epoch 144/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0328 - acc: 0.9883 - val_loss: 0.0701 - val_acc: 0.9848\n",
      "Epoch 145/200\n",
      "44974/44974 [==============================] - 17s 370us/sample - loss: 0.0326 - acc: 0.9879 - val_loss: 0.0776 - val_acc: 0.9830\n",
      "Epoch 146/200\n",
      "44974/44974 [==============================] - 17s 370us/sample - loss: 0.0340 - acc: 0.9877 - val_loss: 0.0701 - val_acc: 0.9848\n",
      "Epoch 147/200\n",
      "44974/44974 [==============================] - 17s 369us/sample - loss: 0.0340 - acc: 0.9875 - val_loss: 0.0711 - val_acc: 0.9840\n",
      "Epoch 148/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0333 - acc: 0.9882 - val_loss: 0.0676 - val_acc: 0.9844\n",
      "Epoch 149/200\n",
      "44974/44974 [==============================] - 17s 370us/sample - loss: 0.0332 - acc: 0.9883 - val_loss: 0.0714 - val_acc: 0.9842\n",
      "Epoch 150/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0321 - acc: 0.9884 - val_loss: 0.0686 - val_acc: 0.9858\n",
      "Epoch 151/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0320 - acc: 0.9886 - val_loss: 0.0678 - val_acc: 0.9848\n",
      "Epoch 152/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0333 - acc: 0.9885 - val_loss: 0.0780 - val_acc: 0.9830\n",
      "Epoch 153/200\n",
      "44974/44974 [==============================] - 17s 370us/sample - loss: 0.0322 - acc: 0.9883 - val_loss: 0.0731 - val_acc: 0.9836\n",
      "Epoch 154/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0324 - acc: 0.9882 - val_loss: 0.0653 - val_acc: 0.9856\n",
      "Epoch 155/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0310 - acc: 0.9894 - val_loss: 0.0714 - val_acc: 0.9844\n",
      "Epoch 156/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0318 - acc: 0.9888 - val_loss: 0.0688 - val_acc: 0.9844\n",
      "Epoch 157/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0314 - acc: 0.9892 - val_loss: 0.0706 - val_acc: 0.9844\n",
      "Epoch 158/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0285 - acc: 0.9902 - val_loss: 0.0733 - val_acc: 0.9832\n",
      "Epoch 159/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0307 - acc: 0.9889 - val_loss: 0.0711 - val_acc: 0.9856\n",
      "Epoch 160/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.0290 - acc: 0.9899 - val_loss: 0.0678 - val_acc: 0.9840\n",
      "Epoch 161/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0280 - acc: 0.9902 - val_loss: 0.0748 - val_acc: 0.9846\n",
      "Epoch 162/200\n",
      "44974/44974 [==============================] - 17s 370us/sample - loss: 0.0297 - acc: 0.9892 - val_loss: 0.0695 - val_acc: 0.9842\n",
      "Epoch 163/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0298 - acc: 0.9894 - val_loss: 0.0675 - val_acc: 0.9862\n",
      "Epoch 164/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0282 - acc: 0.9901 - val_loss: 0.0667 - val_acc: 0.9860\n",
      "Epoch 165/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0286 - acc: 0.9899 - val_loss: 0.0675 - val_acc: 0.9852\n",
      "Epoch 166/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0287 - acc: 0.9894 - val_loss: 0.0692 - val_acc: 0.9858\n",
      "Epoch 167/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0294 - acc: 0.9891 - val_loss: 0.0747 - val_acc: 0.9832\n",
      "Epoch 168/200\n",
      "44974/44974 [==============================] - 17s 370us/sample - loss: 0.0279 - acc: 0.9905 - val_loss: 0.0721 - val_acc: 0.9842\n",
      "Epoch 169/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0297 - acc: 0.9894 - val_loss: 0.0673 - val_acc: 0.9846\n",
      "Epoch 170/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0273 - acc: 0.9906 - val_loss: 0.0705 - val_acc: 0.9850\n",
      "Epoch 171/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0286 - acc: 0.9898 - val_loss: 0.0737 - val_acc: 0.9844\n",
      "Epoch 172/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0289 - acc: 0.9897 - val_loss: 0.0714 - val_acc: 0.9852\n",
      "Epoch 173/200\n",
      "44974/44974 [==============================] - 17s 370us/sample - loss: 0.0283 - acc: 0.9901 - val_loss: 0.0698 - val_acc: 0.9844\n",
      "Epoch 174/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0279 - acc: 0.9905 - val_loss: 0.0720 - val_acc: 0.9850\n",
      "Epoch 175/200\n",
      "44974/44974 [==============================] - 17s 370us/sample - loss: 0.0290 - acc: 0.9897 - val_loss: 0.0678 - val_acc: 0.9864\n",
      "Epoch 176/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0265 - acc: 0.9908 - val_loss: 0.0741 - val_acc: 0.9848\n",
      "Epoch 177/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0278 - acc: 0.9902 - val_loss: 0.0673 - val_acc: 0.9864\n",
      "Epoch 178/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0268 - acc: 0.9906 - val_loss: 0.0725 - val_acc: 0.9846\n",
      "Epoch 179/200\n",
      "44974/44974 [==============================] - 17s 370us/sample - loss: 0.0264 - acc: 0.9904 - val_loss: 0.0707 - val_acc: 0.9852\n",
      "Epoch 180/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0279 - acc: 0.9900 - val_loss: 0.0745 - val_acc: 0.9850\n",
      "Epoch 181/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0266 - acc: 0.9907 - val_loss: 0.0691 - val_acc: 0.9862\n",
      "Epoch 182/200\n",
      "44974/44974 [==============================] - 17s 369us/sample - loss: 0.0265 - acc: 0.9901 - val_loss: 0.0649 - val_acc: 0.9868\n",
      "Epoch 183/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0256 - acc: 0.9907 - val_loss: 0.0694 - val_acc: 0.9856\n",
      "Epoch 184/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0257 - acc: 0.9911 - val_loss: 0.0656 - val_acc: 0.9854\n",
      "Epoch 185/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0252 - acc: 0.9910 - val_loss: 0.0677 - val_acc: 0.9862\n",
      "Epoch 186/200\n",
      "44974/44974 [==============================] - 17s 370us/sample - loss: 0.0247 - acc: 0.9914 - val_loss: 0.0631 - val_acc: 0.9874\n",
      "Epoch 187/200\n",
      "44974/44974 [==============================] - 17s 370us/sample - loss: 0.0251 - acc: 0.9917 - val_loss: 0.0648 - val_acc: 0.9862\n",
      "Epoch 188/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0240 - acc: 0.9915 - val_loss: 0.0620 - val_acc: 0.9876\n",
      "Epoch 189/200\n",
      "44974/44974 [==============================] - 17s 370us/sample - loss: 0.0251 - acc: 0.9911 - val_loss: 0.0676 - val_acc: 0.9872\n",
      "Epoch 190/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0260 - acc: 0.9907 - val_loss: 0.0680 - val_acc: 0.9870\n",
      "Epoch 191/200\n",
      "44974/44974 [==============================] - 17s 370us/sample - loss: 0.0258 - acc: 0.9910 - val_loss: 0.0654 - val_acc: 0.9870\n",
      "Epoch 192/200\n",
      "44974/44974 [==============================] - 17s 370us/sample - loss: 0.0240 - acc: 0.9916 - val_loss: 0.0651 - val_acc: 0.9884\n",
      "Epoch 193/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0253 - acc: 0.9912 - val_loss: 0.0684 - val_acc: 0.9878\n",
      "Epoch 194/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0258 - acc: 0.9912 - val_loss: 0.0618 - val_acc: 0.9884\n",
      "Epoch 195/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0255 - acc: 0.9915 - val_loss: 0.0623 - val_acc: 0.9868\n",
      "Epoch 196/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0251 - acc: 0.9907 - val_loss: 0.0626 - val_acc: 0.9880\n",
      "Epoch 197/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0240 - acc: 0.9916 - val_loss: 0.0656 - val_acc: 0.9878\n",
      "Epoch 198/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0227 - acc: 0.9918 - val_loss: 0.0636 - val_acc: 0.9886\n",
      "Epoch 199/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0240 - acc: 0.9919 - val_loss: 0.0636 - val_acc: 0.9888\n",
      "Epoch 200/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0226 - acc: 0.9922 - val_loss: 0.0694 - val_acc: 0.9876\n"
     ]
    }
   ],
   "source": [
    "model = trainModel(builtWeightSharingModel(drop_out_rate=0.5),[train_headline, train_first_body, train_last_body],[vali_headline, vali_first_body, vali_last_body])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ad63tAQstOsw"
   },
   "outputs": [],
   "source": [
    "def y_to_stance(onehot):\n",
    "    LABELS = ['agree', 'disagree', 'discuss', 'unrelated']\n",
    "    return [LABELS[index] for index in onehot]\n",
    "\n",
    "def evaluate(predicted, actual):\n",
    "    \n",
    "    predicted = y_to_stance(predicted)\n",
    "    print(\"Scores on the test set\")\n",
    "    report_score(actual,predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 88222,
     "status": "ok",
     "timestamp": 1563902707210,
     "user": {
      "displayName": "chuck fu",
      "photoUrl": "",
      "userId": "01740202220584032426"
     },
     "user_tz": 240
    },
    "id": "V5Ybqn3dtOZw",
    "outputId": "8b0c4a95-f53e-4ba9-d5be-37cae48612a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on the test set\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    346    |     6     |     5     |     5     |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |     6     |    72     |     0     |     1     |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |     6     |     1     |    889    |     2     |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |     8     |     1     |    21     |   3629    |\n",
      "-------------------------------------------------------------\n",
      "Score: 2220.25 out of 2253.75\t(98.51358846367165%)\n",
      "Scores on the test set\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    871    |    38     |    354    |    640    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    174    |    67     |    102    |    354    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    949    |    84     |   2305    |   1126    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    959    |    74     |    767    |   16549   |\n",
      "-------------------------------------------------------------\n",
      "Score: 7805.5 out of 11651.25\t(66.99281193005042%)\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict([vali_headline, vali_first_body,vali_last_body])\n",
    "y_classes = pred.argmax(axis=-1)\n",
    "actual = y_vali\n",
    "evaluate(y_classes, actual)\n",
    "\n",
    "\n",
    "\n",
    "pred = model.predict([competition_headline, competition_first_body, competition_last_body])\n",
    "y_classes = pred.argmax(axis=-1)\n",
    "actual = compe['Stance']\n",
    "evaluate(y_classes, actual)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oFd3sOUi79mC"
   },
   "outputs": [],
   "source": [
    "def builtCompositeModel(drop_out_rate=0.5,dense_neuron = 32):\n",
    "    headline_input = Input(shape=(MAX_HEADLINE_LENGTH,), name='headline_input')\n",
    "    first_body_input = Input(shape=(MAX_BODY_TRUNC,), name='first_body_input')\n",
    "    last_body_input = Input(shape=(MAX_BODY_TRUNC,), name='last_body_input')\n",
    "    headline_embedding = Embedding(input_dim=len(tokenizer.word_index)+1,\n",
    "                          output_dim=EMBEDDING_DIM,\n",
    "                          weights = [embedding_matrix], trainable=False, name='headline_word_embedding_layer', \n",
    "                          mask_zero=True)(headline_input)\n",
    "    first_body_embedding = Embedding(input_dim=len(tokenizer.word_index)+1,\n",
    "                          output_dim=EMBEDDING_DIM,\n",
    "                          weights = [embedding_matrix], trainable=False, name='first_body_word_embedding_layer', \n",
    "                          mask_zero=True)(first_body_input)\n",
    "    last_body_embedding = Embedding(input_dim=len(tokenizer.word_index)+1,\n",
    "                          output_dim=EMBEDDING_DIM,\n",
    "                          weights = [embedding_matrix], trainable=False, name='last_bodyword_embedding_layer', \n",
    "                          mask_zero=True)(last_body_input)\n",
    "    encoder_outputs, state_h, state_c = LSTM(LSTM_DIM, return_sequences=False, return_state = True,dropout=0.5,recurrent_dropout = 0.5, name='lstm_layer_encoder')(headline_embedding)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    first_body_outputs, _, _ = LSTM(LSTM_DIM, return_sequences=False, return_state = True,dropout=0.5,recurrent_dropout = 0.5, name='lstm_layer_first_body')(first_body_embedding, initial_state=encoder_states)\n",
    "    last_body_outputs, _, _ = LSTM(LSTM_DIM, return_sequences=False, return_state = True,dropout=0.5,recurrent_dropout = 0.5, name='lstm_layer_last_body')(last_body_embedding, initial_state=encoder_states)\n",
    "    x = tf.keras.layers.concatenate([first_body_outputs, last_body_outputs])\n",
    "    x = Dropout(rate=drop_out_rate, name='dropout_1')(x)\n",
    "    #x = Dense(dense_neuron, activation='relu')(x)\n",
    "    #x = Dropout(rate=drop_out_rate, name='dropout_1')(x)\n",
    "    #x = Dense(dense_neuron, activation='relu')(x)\n",
    "    #x = Dropout(rate=drop_out_rate, name='dropout_2')(x)\n",
    "\n",
    "    # And finally we add the main logistic regression layer\n",
    "    main_output = Dense(4, activation='softmax', name='main_output')(x)\n",
    "    model = Model(inputs=[headline_input, first_body_input, last_body_input], outputs=main_output)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fc84us1IwSwQ"
   },
   "outputs": [],
   "source": [
    "def builtConditionalModel(drop_out_rate=0.5,dense_neuron = 32):\n",
    "    headline_input = Input(shape=(MAX_HEADLINE_LENGTH,), name='headline_input')\n",
    "    first_body_input = Input(shape=(MAX_BODY_TRUNC,), name='first_body_input')\n",
    "    #last_body_input = Input(shape=(MAX_BODY_TRUNC,), name='last_body_input')\n",
    "    e = Embedding(input_dim=len(tokenizer.word_index)+1,\n",
    "                          output_dim=EMBEDDING_DIM,\n",
    "                          weights = [embedding_matrix], trainable=False, name='headline_word_embedding_layer', \n",
    "                          mask_zero=True)\n",
    "    headline_embedding = e(headline_input)\n",
    "    first_body_embedding = e(first_body_input)\n",
    "    #last_body_embedding = e(last_body_input)\n",
    "    encoder_outputs, state_h, state_c = LSTM(LSTM_DIM, return_sequences=False, return_state = True,dropout=0.5,recurrent_dropout = 0.5, name='lstm_layer_encoder')(headline_embedding)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    decoder = LSTM(LSTM_DIM, return_sequences=False, return_state = True,dropout=0.5,recurrent_dropout = 0.5, name='lstm_layer_first_body')\n",
    "    first_body_outputs, _, _ = decoder(first_body_embedding, initial_state=encoder_states)\n",
    "    #last_body_outputs, _, _ = decoder(last_body_embedding, initial_state=encoder_states)\n",
    "    #x = tf.keras.layers.concatenate([first_body_outputs, last_body_outputs])\n",
    "    x = Dropout(rate=drop_out_rate, name='dropout_0')(first_body_outputs)\n",
    "    #x = Dense(dense_neuron, activation='relu')(x)\n",
    "    #x = Dropout(rate=drop_out_rate, name='dropout_1')(x)\n",
    "    #x = Dense(dense_neuron, activation='relu')(x)\n",
    "    #x = Dropout(rate=drop_out_rate, name='dropout_2')(x)\n",
    "\n",
    "    # And finally we add the main logistic regression layer\n",
    "    main_output = Dense(4, activation='softmax', name='main_output')(x)\n",
    "    model = Model(inputs=[headline_input, first_body_input], outputs=main_output)\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1952119,
     "status": "ok",
     "timestamp": 1563905034871,
     "user": {
      "displayName": "chuck fu",
      "photoUrl": "",
      "userId": "01740202220584032426"
     },
     "user_tz": 240
    },
    "id": "X-k2o59Xs44P",
    "outputId": "df2d0746-116c-41f0-ff33-6a1ef946d3e4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 17:31:31.077806 140219664385920 callbacks.py:875] `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_9\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "first_body_input (InputLayer)   [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "headline_input (InputLayer)     [(None, 15)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "headline_word_embedding_layer ( multiple             8825700     headline_input[0][0]             \n",
      "                                                                 first_body_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lstm_layer_encoder (LSTM)       [(None, 300), (None, 721200      headline_word_embedding_layer[0][\n",
      "__________________________________________________________________________________________________\n",
      "lstm_layer_first_body (LSTM)    [(None, 300), (None, 721200      headline_word_embedding_layer[1][\n",
      "                                                                 lstm_layer_encoder[0][1]         \n",
      "                                                                 lstm_layer_encoder[0][2]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_0 (Dropout)             (None, 300)          0           lstm_layer_first_body[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 4)            1204        dropout_0[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 10,269,304\n",
      "Trainable params: 1,443,604\n",
      "Non-trainable params: 8,825,700\n",
      "__________________________________________________________________________________________________\n",
      "Train on 44974 samples, validate on 4998 samples\n",
      "Epoch 1/200\n",
      "44974/44974 [==============================] - 13s 282us/sample - loss: 0.8469 - acc: 0.7133 - val_loss: 0.7681 - val_acc: 0.7321\n",
      "Epoch 2/200\n",
      "44974/44974 [==============================] - 10s 217us/sample - loss: 0.7781 - acc: 0.7298 - val_loss: 0.7455 - val_acc: 0.7321\n",
      "Epoch 3/200\n",
      "44974/44974 [==============================] - 10s 215us/sample - loss: 0.7461 - acc: 0.7364 - val_loss: 0.6956 - val_acc: 0.7597\n",
      "Epoch 4/200\n",
      "44974/44974 [==============================] - 10s 214us/sample - loss: 0.7098 - acc: 0.7486 - val_loss: 0.6714 - val_acc: 0.7741\n",
      "Epoch 5/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.6838 - acc: 0.7594 - val_loss: 0.6346 - val_acc: 0.7849\n",
      "Epoch 6/200\n",
      "44974/44974 [==============================] - 9s 211us/sample - loss: 0.6544 - acc: 0.7687 - val_loss: 0.5907 - val_acc: 0.7897\n",
      "Epoch 7/200\n",
      "44974/44974 [==============================] - 9s 211us/sample - loss: 0.6092 - acc: 0.7770 - val_loss: 0.5219 - val_acc: 0.8023\n",
      "Epoch 8/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.5561 - acc: 0.7884 - val_loss: 0.4677 - val_acc: 0.8145\n",
      "Epoch 9/200\n",
      "44974/44974 [==============================] - 10s 214us/sample - loss: 0.5138 - acc: 0.8007 - val_loss: 0.4249 - val_acc: 0.8341\n",
      "Epoch 10/200\n",
      "44974/44974 [==============================] - 13s 290us/sample - loss: 0.4767 - acc: 0.8156 - val_loss: 0.3931 - val_acc: 0.8429\n",
      "Epoch 11/200\n",
      "44974/44974 [==============================] - 10s 216us/sample - loss: 0.4434 - acc: 0.8280 - val_loss: 0.3565 - val_acc: 0.8571\n",
      "Epoch 12/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.4216 - acc: 0.8359 - val_loss: 0.3489 - val_acc: 0.8627\n",
      "Epoch 13/200\n",
      "44974/44974 [==============================] - 10s 214us/sample - loss: 0.3998 - acc: 0.8458 - val_loss: 0.3199 - val_acc: 0.8727\n",
      "Epoch 14/200\n",
      "44974/44974 [==============================] - 10s 215us/sample - loss: 0.3806 - acc: 0.8527 - val_loss: 0.3047 - val_acc: 0.8790\n",
      "Epoch 15/200\n",
      "44974/44974 [==============================] - 10s 214us/sample - loss: 0.3604 - acc: 0.8607 - val_loss: 0.2829 - val_acc: 0.8898\n",
      "Epoch 16/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.3439 - acc: 0.8689 - val_loss: 0.2674 - val_acc: 0.8976\n",
      "Epoch 17/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.3298 - acc: 0.8732 - val_loss: 0.2548 - val_acc: 0.9014\n",
      "Epoch 18/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.3174 - acc: 0.8794 - val_loss: 0.2415 - val_acc: 0.9112\n",
      "Epoch 19/200\n",
      "44974/44974 [==============================] - 10s 214us/sample - loss: 0.3007 - acc: 0.8849 - val_loss: 0.2423 - val_acc: 0.9108\n",
      "Epoch 20/200\n",
      "44974/44974 [==============================] - 10s 214us/sample - loss: 0.2889 - acc: 0.8894 - val_loss: 0.2262 - val_acc: 0.9136\n",
      "Epoch 21/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.2736 - acc: 0.8963 - val_loss: 0.2079 - val_acc: 0.9204\n",
      "Epoch 22/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.2652 - acc: 0.8988 - val_loss: 0.2018 - val_acc: 0.9222\n",
      "Epoch 23/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.2578 - acc: 0.9015 - val_loss: 0.1977 - val_acc: 0.9268\n",
      "Epoch 24/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.2437 - acc: 0.9077 - val_loss: 0.1872 - val_acc: 0.9284\n",
      "Epoch 25/200\n",
      "44974/44974 [==============================] - 10s 214us/sample - loss: 0.2317 - acc: 0.9120 - val_loss: 0.1772 - val_acc: 0.9340\n",
      "Epoch 26/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.2244 - acc: 0.9144 - val_loss: 0.1715 - val_acc: 0.9350\n",
      "Epoch 27/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.2181 - acc: 0.9181 - val_loss: 0.1680 - val_acc: 0.9374\n",
      "Epoch 28/200\n",
      "44974/44974 [==============================] - 10s 214us/sample - loss: 0.2147 - acc: 0.9182 - val_loss: 0.1568 - val_acc: 0.9450\n",
      "Epoch 29/200\n",
      "44974/44974 [==============================] - 10s 215us/sample - loss: 0.2053 - acc: 0.9225 - val_loss: 0.1569 - val_acc: 0.9436\n",
      "Epoch 30/200\n",
      "44974/44974 [==============================] - 10s 218us/sample - loss: 0.2004 - acc: 0.9253 - val_loss: 0.1639 - val_acc: 0.9400\n",
      "Epoch 31/200\n",
      "44974/44974 [==============================] - 10s 219us/sample - loss: 0.1934 - acc: 0.9273 - val_loss: 0.1425 - val_acc: 0.9466\n",
      "Epoch 32/200\n",
      "44974/44974 [==============================] - 10s 224us/sample - loss: 0.1873 - acc: 0.9290 - val_loss: 0.1397 - val_acc: 0.9500\n",
      "Epoch 33/200\n",
      "44974/44974 [==============================] - 11s 239us/sample - loss: 0.1839 - acc: 0.9318 - val_loss: 0.1409 - val_acc: 0.9480\n",
      "Epoch 34/200\n",
      "44974/44974 [==============================] - 11s 240us/sample - loss: 0.1754 - acc: 0.9331 - val_loss: 0.1455 - val_acc: 0.9468\n",
      "Epoch 35/200\n",
      "44974/44974 [==============================] - 12s 256us/sample - loss: 0.1747 - acc: 0.9341 - val_loss: 0.1343 - val_acc: 0.9522\n",
      "Epoch 36/200\n",
      "44974/44974 [==============================] - 11s 254us/sample - loss: 0.1675 - acc: 0.9362 - val_loss: 0.1314 - val_acc: 0.9524\n",
      "Epoch 37/200\n",
      "44974/44974 [==============================] - 12s 265us/sample - loss: 0.1640 - acc: 0.9374 - val_loss: 0.1247 - val_acc: 0.9552\n",
      "Epoch 38/200\n",
      "44974/44974 [==============================] - 12s 263us/sample - loss: 0.1633 - acc: 0.9394 - val_loss: 0.1208 - val_acc: 0.9564\n",
      "Epoch 39/200\n",
      "44974/44974 [==============================] - 12s 260us/sample - loss: 0.1567 - acc: 0.9412 - val_loss: 0.1204 - val_acc: 0.9560\n",
      "Epoch 40/200\n",
      "44974/44974 [==============================] - 12s 262us/sample - loss: 0.1524 - acc: 0.9416 - val_loss: 0.1156 - val_acc: 0.9580\n",
      "Epoch 41/200\n",
      "44974/44974 [==============================] - 12s 269us/sample - loss: 0.1509 - acc: 0.9443 - val_loss: 0.1099 - val_acc: 0.9594\n",
      "Epoch 42/200\n",
      "44974/44974 [==============================] - 10s 217us/sample - loss: 0.1427 - acc: 0.9468 - val_loss: 0.1109 - val_acc: 0.9602\n",
      "Epoch 43/200\n",
      "44974/44974 [==============================] - 10s 215us/sample - loss: 0.1440 - acc: 0.9458 - val_loss: 0.1088 - val_acc: 0.9598\n",
      "Epoch 44/200\n",
      "44974/44974 [==============================] - 10s 214us/sample - loss: 0.1354 - acc: 0.9488 - val_loss: 0.1097 - val_acc: 0.9610\n",
      "Epoch 45/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.1335 - acc: 0.9493 - val_loss: 0.1022 - val_acc: 0.9632\n",
      "Epoch 46/200\n",
      "44974/44974 [==============================] - 10s 214us/sample - loss: 0.1354 - acc: 0.9489 - val_loss: 0.1078 - val_acc: 0.9618\n",
      "Epoch 47/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.1306 - acc: 0.9503 - val_loss: 0.1078 - val_acc: 0.9610\n",
      "Epoch 48/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.1277 - acc: 0.9520 - val_loss: 0.1007 - val_acc: 0.9644\n",
      "Epoch 49/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.1267 - acc: 0.9521 - val_loss: 0.0993 - val_acc: 0.9648\n",
      "Epoch 50/200\n",
      "44974/44974 [==============================] - 10s 214us/sample - loss: 0.1221 - acc: 0.9538 - val_loss: 0.0980 - val_acc: 0.9652\n",
      "Epoch 51/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.1212 - acc: 0.9549 - val_loss: 0.0974 - val_acc: 0.9654\n",
      "Epoch 52/200\n",
      "44974/44974 [==============================] - 10s 214us/sample - loss: 0.1177 - acc: 0.9549 - val_loss: 0.0987 - val_acc: 0.9670\n",
      "Epoch 53/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.1163 - acc: 0.9570 - val_loss: 0.1015 - val_acc: 0.9652\n",
      "Epoch 54/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.1140 - acc: 0.9574 - val_loss: 0.0905 - val_acc: 0.9682\n",
      "Epoch 55/200\n",
      "44974/44974 [==============================] - 10s 214us/sample - loss: 0.1121 - acc: 0.9571 - val_loss: 0.0942 - val_acc: 0.9652\n",
      "Epoch 56/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.1122 - acc: 0.9573 - val_loss: 0.0926 - val_acc: 0.9678\n",
      "Epoch 57/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.1090 - acc: 0.9579 - val_loss: 0.0919 - val_acc: 0.9680\n",
      "Epoch 58/200\n",
      "44974/44974 [==============================] - 10s 214us/sample - loss: 0.1061 - acc: 0.9598 - val_loss: 0.0935 - val_acc: 0.9660\n",
      "Epoch 59/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.1051 - acc: 0.9604 - val_loss: 0.0884 - val_acc: 0.9702\n",
      "Epoch 60/200\n",
      "44974/44974 [==============================] - 10s 215us/sample - loss: 0.1053 - acc: 0.9604 - val_loss: 0.0918 - val_acc: 0.9674\n",
      "Epoch 61/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.1018 - acc: 0.9609 - val_loss: 0.0871 - val_acc: 0.9702\n",
      "Epoch 62/200\n",
      "44974/44974 [==============================] - 10s 214us/sample - loss: 0.0989 - acc: 0.9620 - val_loss: 0.0861 - val_acc: 0.9718\n",
      "Epoch 63/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0995 - acc: 0.9631 - val_loss: 0.0841 - val_acc: 0.9706\n",
      "Epoch 64/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0951 - acc: 0.9640 - val_loss: 0.0817 - val_acc: 0.9732\n",
      "Epoch 65/200\n",
      "44974/44974 [==============================] - 10s 214us/sample - loss: 0.0936 - acc: 0.9643 - val_loss: 0.0843 - val_acc: 0.9718\n",
      "Epoch 66/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0957 - acc: 0.9644 - val_loss: 0.0833 - val_acc: 0.9718\n",
      "Epoch 67/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0969 - acc: 0.9631 - val_loss: 0.0852 - val_acc: 0.9700\n",
      "Epoch 68/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0916 - acc: 0.9656 - val_loss: 0.0785 - val_acc: 0.9734\n",
      "Epoch 69/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0923 - acc: 0.9651 - val_loss: 0.0771 - val_acc: 0.9736\n",
      "Epoch 70/200\n",
      "44974/44974 [==============================] - 10s 215us/sample - loss: 0.0885 - acc: 0.9662 - val_loss: 0.0803 - val_acc: 0.9740\n",
      "Epoch 71/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0881 - acc: 0.9666 - val_loss: 0.0761 - val_acc: 0.9754\n",
      "Epoch 72/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0830 - acc: 0.9683 - val_loss: 0.0772 - val_acc: 0.9768\n",
      "Epoch 73/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0835 - acc: 0.9686 - val_loss: 0.0796 - val_acc: 0.9746\n",
      "Epoch 74/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0822 - acc: 0.9696 - val_loss: 0.0741 - val_acc: 0.9766\n",
      "Epoch 75/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0843 - acc: 0.9686 - val_loss: 0.0703 - val_acc: 0.9776\n",
      "Epoch 76/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0829 - acc: 0.9693 - val_loss: 0.0702 - val_acc: 0.9768\n",
      "Epoch 77/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0830 - acc: 0.9682 - val_loss: 0.0773 - val_acc: 0.9756\n",
      "Epoch 78/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0799 - acc: 0.9699 - val_loss: 0.0721 - val_acc: 0.9770\n",
      "Epoch 79/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0794 - acc: 0.9705 - val_loss: 0.0723 - val_acc: 0.9768\n",
      "Epoch 80/200\n",
      "44974/44974 [==============================] - 10s 214us/sample - loss: 0.0772 - acc: 0.9701 - val_loss: 0.0712 - val_acc: 0.9774\n",
      "Epoch 81/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0773 - acc: 0.9706 - val_loss: 0.0704 - val_acc: 0.9784\n",
      "Epoch 82/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0775 - acc: 0.9706 - val_loss: 0.0751 - val_acc: 0.9762\n",
      "Epoch 83/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0750 - acc: 0.9720 - val_loss: 0.0770 - val_acc: 0.9746\n",
      "Epoch 84/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0716 - acc: 0.9729 - val_loss: 0.0761 - val_acc: 0.9736\n",
      "Epoch 85/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0715 - acc: 0.9731 - val_loss: 0.0724 - val_acc: 0.9764\n",
      "Epoch 86/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0748 - acc: 0.9726 - val_loss: 0.0702 - val_acc: 0.9754\n",
      "Epoch 87/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0700 - acc: 0.9738 - val_loss: 0.0696 - val_acc: 0.9776\n",
      "Epoch 88/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0715 - acc: 0.9728 - val_loss: 0.0702 - val_acc: 0.9768\n",
      "Epoch 89/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0705 - acc: 0.9737 - val_loss: 0.0665 - val_acc: 0.9786\n",
      "Epoch 90/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0691 - acc: 0.9743 - val_loss: 0.0719 - val_acc: 0.9770\n",
      "Epoch 91/200\n",
      "44974/44974 [==============================] - 9s 211us/sample - loss: 0.0712 - acc: 0.9732 - val_loss: 0.0719 - val_acc: 0.9770\n",
      "Epoch 92/200\n",
      "44974/44974 [==============================] - 10s 211us/sample - loss: 0.0665 - acc: 0.9749 - val_loss: 0.0720 - val_acc: 0.9768\n",
      "Epoch 93/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0699 - acc: 0.9741 - val_loss: 0.0665 - val_acc: 0.9780\n",
      "Epoch 94/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0648 - acc: 0.9755 - val_loss: 0.0697 - val_acc: 0.9774\n",
      "Epoch 95/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0659 - acc: 0.9750 - val_loss: 0.0692 - val_acc: 0.9788\n",
      "Epoch 96/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0640 - acc: 0.9757 - val_loss: 0.0720 - val_acc: 0.9774\n",
      "Epoch 97/200\n",
      "44974/44974 [==============================] - 10s 211us/sample - loss: 0.0627 - acc: 0.9763 - val_loss: 0.0673 - val_acc: 0.9786\n",
      "Epoch 98/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0624 - acc: 0.9770 - val_loss: 0.0700 - val_acc: 0.9782\n",
      "Epoch 99/200\n",
      "44974/44974 [==============================] - 9s 211us/sample - loss: 0.0630 - acc: 0.9771 - val_loss: 0.0654 - val_acc: 0.9792\n",
      "Epoch 100/200\n",
      "44974/44974 [==============================] - 9s 211us/sample - loss: 0.0631 - acc: 0.9760 - val_loss: 0.0714 - val_acc: 0.9786\n",
      "Epoch 101/200\n",
      "44974/44974 [==============================] - 9s 211us/sample - loss: 0.0653 - acc: 0.9751 - val_loss: 0.0702 - val_acc: 0.9770\n",
      "Epoch 102/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0628 - acc: 0.9769 - val_loss: 0.0672 - val_acc: 0.9778\n",
      "Epoch 103/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0595 - acc: 0.9781 - val_loss: 0.0659 - val_acc: 0.9790\n",
      "Epoch 104/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0615 - acc: 0.9772 - val_loss: 0.0679 - val_acc: 0.9784\n",
      "Epoch 105/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0613 - acc: 0.9769 - val_loss: 0.0651 - val_acc: 0.9802\n",
      "Epoch 106/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0566 - acc: 0.9790 - val_loss: 0.0693 - val_acc: 0.9790\n",
      "Epoch 107/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0571 - acc: 0.9785 - val_loss: 0.0697 - val_acc: 0.9794\n",
      "Epoch 108/200\n",
      "44974/44974 [==============================] - 10s 214us/sample - loss: 0.0584 - acc: 0.9788 - val_loss: 0.0660 - val_acc: 0.9804\n",
      "Epoch 109/200\n",
      "44974/44974 [==============================] - 10s 211us/sample - loss: 0.0569 - acc: 0.9784 - val_loss: 0.0638 - val_acc: 0.9808\n",
      "Epoch 110/200\n",
      "44974/44974 [==============================] - 10s 214us/sample - loss: 0.0593 - acc: 0.9777 - val_loss: 0.0609 - val_acc: 0.9828\n",
      "Epoch 111/200\n",
      "44974/44974 [==============================] - 10s 211us/sample - loss: 0.0582 - acc: 0.9788 - val_loss: 0.0651 - val_acc: 0.9804\n",
      "Epoch 112/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0555 - acc: 0.9791 - val_loss: 0.0619 - val_acc: 0.9820\n",
      "Epoch 113/200\n",
      "44974/44974 [==============================] - 9s 211us/sample - loss: 0.0540 - acc: 0.9802 - val_loss: 0.0642 - val_acc: 0.9810\n",
      "Epoch 114/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0553 - acc: 0.9798 - val_loss: 0.0587 - val_acc: 0.9828\n",
      "Epoch 115/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0536 - acc: 0.9796 - val_loss: 0.0639 - val_acc: 0.9804\n",
      "Epoch 116/200\n",
      "44974/44974 [==============================] - 10s 211us/sample - loss: 0.0560 - acc: 0.9791 - val_loss: 0.0649 - val_acc: 0.9808\n",
      "Epoch 117/200\n",
      "44974/44974 [==============================] - 10s 211us/sample - loss: 0.0526 - acc: 0.9802 - val_loss: 0.0662 - val_acc: 0.9802\n",
      "Epoch 118/200\n",
      "44974/44974 [==============================] - 10s 211us/sample - loss: 0.0555 - acc: 0.9796 - val_loss: 0.0669 - val_acc: 0.9808\n",
      "Epoch 119/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0558 - acc: 0.9799 - val_loss: 0.0644 - val_acc: 0.9812\n",
      "Epoch 120/200\n",
      "44974/44974 [==============================] - 10s 211us/sample - loss: 0.0541 - acc: 0.9801 - val_loss: 0.0627 - val_acc: 0.9816\n",
      "Epoch 121/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0515 - acc: 0.9806 - val_loss: 0.0633 - val_acc: 0.9824\n",
      "Epoch 122/200\n",
      "44974/44974 [==============================] - 9s 211us/sample - loss: 0.0497 - acc: 0.9821 - val_loss: 0.0630 - val_acc: 0.9828\n",
      "Epoch 123/200\n",
      "44974/44974 [==============================] - 9s 211us/sample - loss: 0.0514 - acc: 0.9808 - val_loss: 0.0603 - val_acc: 0.9832\n",
      "Epoch 124/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0518 - acc: 0.9813 - val_loss: 0.0642 - val_acc: 0.9818\n",
      "Epoch 125/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0508 - acc: 0.9810 - val_loss: 0.0609 - val_acc: 0.9828\n",
      "Epoch 126/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0496 - acc: 0.9815 - val_loss: 0.0655 - val_acc: 0.9822\n",
      "Epoch 127/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0486 - acc: 0.9817 - val_loss: 0.0628 - val_acc: 0.9818\n",
      "Epoch 128/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0480 - acc: 0.9823 - val_loss: 0.0619 - val_acc: 0.9832\n",
      "Epoch 129/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0507 - acc: 0.9809 - val_loss: 0.0626 - val_acc: 0.9826\n",
      "Epoch 130/200\n",
      "44974/44974 [==============================] - 10s 214us/sample - loss: 0.0508 - acc: 0.9814 - val_loss: 0.0589 - val_acc: 0.9842\n",
      "Epoch 131/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0478 - acc: 0.9824 - val_loss: 0.0619 - val_acc: 0.9844\n",
      "Epoch 132/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0484 - acc: 0.9815 - val_loss: 0.0581 - val_acc: 0.9850\n",
      "Epoch 133/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0465 - acc: 0.9832 - val_loss: 0.0631 - val_acc: 0.9828\n",
      "Epoch 134/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0479 - acc: 0.9820 - val_loss: 0.0605 - val_acc: 0.9840\n",
      "Epoch 135/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0470 - acc: 0.9827 - val_loss: 0.0613 - val_acc: 0.9834\n",
      "Epoch 136/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0477 - acc: 0.9831 - val_loss: 0.0575 - val_acc: 0.9846\n",
      "Epoch 137/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0464 - acc: 0.9832 - val_loss: 0.0612 - val_acc: 0.9832\n",
      "Epoch 138/200\n",
      "44974/44974 [==============================] - 10s 214us/sample - loss: 0.0454 - acc: 0.9836 - val_loss: 0.0592 - val_acc: 0.9830\n",
      "Epoch 139/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0454 - acc: 0.9841 - val_loss: 0.0559 - val_acc: 0.9848\n",
      "Epoch 140/200\n",
      "44974/44974 [==============================] - 10s 214us/sample - loss: 0.0463 - acc: 0.9832 - val_loss: 0.0634 - val_acc: 0.9826\n",
      "Epoch 141/200\n",
      "44974/44974 [==============================] - 10s 214us/sample - loss: 0.0438 - acc: 0.9839 - val_loss: 0.0566 - val_acc: 0.9844\n",
      "Epoch 142/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0461 - acc: 0.9830 - val_loss: 0.0592 - val_acc: 0.9828\n",
      "Epoch 143/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0445 - acc: 0.9839 - val_loss: 0.0584 - val_acc: 0.9844\n",
      "Epoch 144/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0440 - acc: 0.9841 - val_loss: 0.0603 - val_acc: 0.9834\n",
      "Epoch 145/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0441 - acc: 0.9837 - val_loss: 0.0664 - val_acc: 0.9818\n",
      "Epoch 146/200\n",
      "44974/44974 [==============================] - 10s 214us/sample - loss: 0.0431 - acc: 0.9848 - val_loss: 0.0607 - val_acc: 0.9840\n",
      "Epoch 147/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0442 - acc: 0.9842 - val_loss: 0.0607 - val_acc: 0.9836\n",
      "Epoch 148/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0433 - acc: 0.9839 - val_loss: 0.0599 - val_acc: 0.9838\n",
      "Epoch 149/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0435 - acc: 0.9838 - val_loss: 0.0646 - val_acc: 0.9828\n",
      "Epoch 150/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0439 - acc: 0.9841 - val_loss: 0.0640 - val_acc: 0.9830\n",
      "Epoch 151/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0433 - acc: 0.9839 - val_loss: 0.0588 - val_acc: 0.9832\n",
      "Epoch 152/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0414 - acc: 0.9849 - val_loss: 0.0575 - val_acc: 0.9834\n",
      "Epoch 153/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0426 - acc: 0.9850 - val_loss: 0.0597 - val_acc: 0.9832\n",
      "Epoch 154/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0376 - acc: 0.9863 - val_loss: 0.0643 - val_acc: 0.9836\n",
      "Epoch 155/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0418 - acc: 0.9846 - val_loss: 0.0590 - val_acc: 0.9838\n",
      "Epoch 156/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0410 - acc: 0.9850 - val_loss: 0.0658 - val_acc: 0.9828\n",
      "Epoch 157/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0404 - acc: 0.9851 - val_loss: 0.0625 - val_acc: 0.9836\n",
      "Epoch 158/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0423 - acc: 0.9845 - val_loss: 0.0597 - val_acc: 0.9838\n",
      "Epoch 159/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0397 - acc: 0.9855 - val_loss: 0.0645 - val_acc: 0.9836\n",
      "Epoch 160/200\n",
      "44974/44974 [==============================] - 10s 211us/sample - loss: 0.0389 - acc: 0.9852 - val_loss: 0.0644 - val_acc: 0.9836\n",
      "Epoch 161/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0389 - acc: 0.9862 - val_loss: 0.0626 - val_acc: 0.9840\n",
      "Epoch 162/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0396 - acc: 0.9859 - val_loss: 0.0593 - val_acc: 0.9854\n",
      "Epoch 163/200\n",
      "44974/44974 [==============================] - 10s 214us/sample - loss: 0.0382 - acc: 0.9858 - val_loss: 0.0652 - val_acc: 0.9842\n",
      "Epoch 164/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0388 - acc: 0.9864 - val_loss: 0.0614 - val_acc: 0.9848\n",
      "Epoch 165/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0404 - acc: 0.9851 - val_loss: 0.0640 - val_acc: 0.9838\n",
      "Epoch 166/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0395 - acc: 0.9857 - val_loss: 0.0708 - val_acc: 0.9826\n",
      "Epoch 167/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0402 - acc: 0.9851 - val_loss: 0.0621 - val_acc: 0.9834\n",
      "Epoch 168/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0388 - acc: 0.9861 - val_loss: 0.0656 - val_acc: 0.9836\n",
      "Epoch 169/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0399 - acc: 0.9857 - val_loss: 0.0629 - val_acc: 0.9842\n",
      "Epoch 170/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0388 - acc: 0.9861 - val_loss: 0.0614 - val_acc: 0.9848\n",
      "Epoch 171/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0385 - acc: 0.9863 - val_loss: 0.0639 - val_acc: 0.9840\n",
      "Epoch 172/200\n",
      "44974/44974 [==============================] - 10s 214us/sample - loss: 0.0383 - acc: 0.9866 - val_loss: 0.0631 - val_acc: 0.9844\n",
      "Epoch 173/200\n",
      "44974/44974 [==============================] - 10s 214us/sample - loss: 0.0378 - acc: 0.9867 - val_loss: 0.0596 - val_acc: 0.9850\n",
      "Epoch 174/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0357 - acc: 0.9874 - val_loss: 0.0600 - val_acc: 0.9840\n",
      "Epoch 175/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0369 - acc: 0.9869 - val_loss: 0.0616 - val_acc: 0.9834\n",
      "Epoch 176/200\n",
      "44974/44974 [==============================] - 9s 211us/sample - loss: 0.0356 - acc: 0.9871 - val_loss: 0.0601 - val_acc: 0.9844\n",
      "Epoch 177/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0389 - acc: 0.9856 - val_loss: 0.0626 - val_acc: 0.9846\n",
      "Epoch 178/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0362 - acc: 0.9874 - val_loss: 0.0624 - val_acc: 0.9848\n",
      "Epoch 179/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0349 - acc: 0.9872 - val_loss: 0.0619 - val_acc: 0.9850\n",
      "Epoch 180/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0365 - acc: 0.9873 - val_loss: 0.0606 - val_acc: 0.9846\n",
      "Epoch 181/200\n",
      "44974/44974 [==============================] - 10s 211us/sample - loss: 0.0344 - acc: 0.9876 - val_loss: 0.0619 - val_acc: 0.9846\n",
      "Epoch 182/200\n",
      "44974/44974 [==============================] - 10s 211us/sample - loss: 0.0352 - acc: 0.9875 - val_loss: 0.0610 - val_acc: 0.9842\n",
      "Epoch 183/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0344 - acc: 0.9874 - val_loss: 0.0588 - val_acc: 0.9854\n",
      "Epoch 184/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0342 - acc: 0.9868 - val_loss: 0.0584 - val_acc: 0.9860\n",
      "Epoch 185/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0346 - acc: 0.9875 - val_loss: 0.0604 - val_acc: 0.9846\n",
      "Epoch 186/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0345 - acc: 0.9868 - val_loss: 0.0645 - val_acc: 0.9844\n",
      "Epoch 187/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0366 - acc: 0.9870 - val_loss: 0.0589 - val_acc: 0.9858\n",
      "Epoch 188/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0359 - acc: 0.9873 - val_loss: 0.0576 - val_acc: 0.9856\n",
      "Epoch 189/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0345 - acc: 0.9876 - val_loss: 0.0665 - val_acc: 0.9850\n",
      "Epoch 190/200\n",
      "44974/44974 [==============================] - 10s 211us/sample - loss: 0.0332 - acc: 0.9882 - val_loss: 0.0620 - val_acc: 0.9852\n",
      "Epoch 191/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0352 - acc: 0.9874 - val_loss: 0.0608 - val_acc: 0.9850\n",
      "Epoch 192/200\n",
      "44974/44974 [==============================] - 9s 211us/sample - loss: 0.0324 - acc: 0.9885 - val_loss: 0.0615 - val_acc: 0.9844\n",
      "Epoch 193/200\n",
      "44974/44974 [==============================] - 10s 211us/sample - loss: 0.0319 - acc: 0.9881 - val_loss: 0.0644 - val_acc: 0.9846\n",
      "Epoch 194/200\n",
      "44974/44974 [==============================] - 10s 211us/sample - loss: 0.0336 - acc: 0.9879 - val_loss: 0.0626 - val_acc: 0.9842\n",
      "Epoch 195/200\n",
      "44974/44974 [==============================] - 10s 211us/sample - loss: 0.0332 - acc: 0.9882 - val_loss: 0.0612 - val_acc: 0.9848\n",
      "Epoch 196/200\n",
      "44974/44974 [==============================] - 10s 213us/sample - loss: 0.0331 - acc: 0.9881 - val_loss: 0.0642 - val_acc: 0.9840\n",
      "Epoch 197/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0356 - acc: 0.9877 - val_loss: 0.0595 - val_acc: 0.9858\n",
      "Epoch 198/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0352 - acc: 0.9878 - val_loss: 0.0581 - val_acc: 0.9856\n",
      "Epoch 199/200\n",
      "44974/44974 [==============================] - 10s 212us/sample - loss: 0.0355 - acc: 0.9877 - val_loss: 0.0610 - val_acc: 0.9856\n",
      "Epoch 200/200\n",
      "44974/44974 [==============================] - 10s 215us/sample - loss: 0.0334 - acc: 0.9878 - val_loss: 0.0574 - val_acc: 0.9866\n"
     ]
    }
   ],
   "source": [
    "model = trainModel(builtConditionalModel(drop_out_rate=0.5),[train_headline, train_first_body],[vali_headline, vali_first_body])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 53543,
     "status": "ok",
     "timestamp": 1563905179752,
     "user": {
      "displayName": "chuck fu",
      "photoUrl": "",
      "userId": "01740202220584032426"
     },
     "user_tz": 240
    },
    "id": "NiN6IX8sAmnK",
    "outputId": "a8dc3503-2eb6-498f-87ab-d90192aef563"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on the test set\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    343    |    12     |     6     |     1     |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |     8     |    71     |     0     |     0     |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |     3     |     1     |    889    |     5     |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |     7     |     1     |    23     |   3628    |\n",
      "-------------------------------------------------------------\n",
      "Score: 2217.5 out of 2253.75\t(98.39156960621187%)\n",
      "Scores on the test set\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    909    |    41     |    318    |    635    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    196    |    80     |    98     |    323    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    807    |    80     |   2284    |   1293    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |    999    |    119    |    794    |   16437   |\n",
      "-------------------------------------------------------------\n",
      "Score: 7767.25 out of 11651.25\t(66.66452097414441%)\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict([vali_headline, vali_first_body,vali_last_body])\n",
    "y_classes = pred.argmax(axis=-1)\n",
    "actual = y_vali\n",
    "evaluate(y_classes, actual)\n",
    "\n",
    "\n",
    "\n",
    "pred = model.predict([competition_headline, competition_first_body, competition_last_body])\n",
    "y_classes = pred.argmax(axis=-1)\n",
    "actual = compe['Stance']\n",
    "evaluate(y_classes, actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3391943,
     "status": "ok",
     "timestamp": 1563908599047,
     "user": {
      "displayName": "chuck fu",
      "photoUrl": "",
      "userId": "01740202220584032426"
     },
     "user_tz": 240
    },
    "id": "u0BfYLMMtB8I",
    "outputId": "f8a63837-56e3-4302-fd9e-265b3c153524"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 18:07:02.809592 140219664385920 callbacks.py:875] `period` argument is deprecated. Please use `save_freq` to specify the frequency in number of samples seen.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "headline_input (InputLayer)     [(None, 15)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "first_body_input (InputLayer)   [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "headline_word_embedding_layer ( (None, 15, 300)      8825700     headline_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "last_body_input (InputLayer)    [(None, 50)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "first_body_word_embedding_layer (None, 50, 300)      8825700     first_body_input[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "lstm_layer_encoder (LSTM)       [(None, 300), (None, 721200      headline_word_embedding_layer[0][\n",
      "__________________________________________________________________________________________________\n",
      "last_bodyword_embedding_layer ( (None, 50, 300)      8825700     last_body_input[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_layer_first_body (LSTM)    [(None, 300), (None, 721200      first_body_word_embedding_layer[0\n",
      "                                                                 lstm_layer_encoder[0][1]         \n",
      "                                                                 lstm_layer_encoder[0][2]         \n",
      "__________________________________________________________________________________________________\n",
      "lstm_layer_last_body (LSTM)     [(None, 300), (None, 721200      last_bodyword_embedding_layer[0][\n",
      "                                                                 lstm_layer_encoder[0][1]         \n",
      "                                                                 lstm_layer_encoder[0][2]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 600)          0           lstm_layer_first_body[0][0]      \n",
      "                                                                 lstm_layer_last_body[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 600)          0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 4)            2404        dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 28,643,104\n",
      "Trainable params: 2,166,004\n",
      "Non-trainable params: 26,477,100\n",
      "__________________________________________________________________________________________________\n",
      "Train on 44974 samples, validate on 4998 samples\n",
      "Epoch 1/200\n",
      "44974/44974 [==============================] - 21s 463us/sample - loss: 0.8242 - acc: 0.7197 - val_loss: 0.7465 - val_acc: 0.7321\n",
      "Epoch 2/200\n",
      "44974/44974 [==============================] - 17s 378us/sample - loss: 0.7519 - acc: 0.7332 - val_loss: 0.7075 - val_acc: 0.7417\n",
      "Epoch 3/200\n",
      "44974/44974 [==============================] - 17s 378us/sample - loss: 0.7143 - acc: 0.7441 - val_loss: 0.6559 - val_acc: 0.7683\n",
      "Epoch 4/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.6783 - acc: 0.7581 - val_loss: 0.6134 - val_acc: 0.7843\n",
      "Epoch 5/200\n",
      "44974/44974 [==============================] - 17s 375us/sample - loss: 0.6372 - acc: 0.7697 - val_loss: 0.5578 - val_acc: 0.7955\n",
      "Epoch 6/200\n",
      "44974/44974 [==============================] - 17s 375us/sample - loss: 0.5887 - acc: 0.7819 - val_loss: 0.5034 - val_acc: 0.8109\n",
      "Epoch 7/200\n",
      "44974/44974 [==============================] - 17s 376us/sample - loss: 0.5408 - acc: 0.7961 - val_loss: 0.4529 - val_acc: 0.8293\n",
      "Epoch 8/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.4976 - acc: 0.8110 - val_loss: 0.4107 - val_acc: 0.8387\n",
      "Epoch 9/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.4650 - acc: 0.8209 - val_loss: 0.3886 - val_acc: 0.8447\n",
      "Epoch 10/200\n",
      "44974/44974 [==============================] - 22s 479us/sample - loss: 0.4341 - acc: 0.8322 - val_loss: 0.3599 - val_acc: 0.8589\n",
      "Epoch 11/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.4052 - acc: 0.8415 - val_loss: 0.3369 - val_acc: 0.8683\n",
      "Epoch 12/200\n",
      "44974/44974 [==============================] - 17s 376us/sample - loss: 0.3820 - acc: 0.8527 - val_loss: 0.3128 - val_acc: 0.8754\n",
      "Epoch 13/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.3639 - acc: 0.8593 - val_loss: 0.2980 - val_acc: 0.8824\n",
      "Epoch 14/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.3416 - acc: 0.8684 - val_loss: 0.2819 - val_acc: 0.8892\n",
      "Epoch 15/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.3227 - acc: 0.8767 - val_loss: 0.2568 - val_acc: 0.8976\n",
      "Epoch 16/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.3037 - acc: 0.8817 - val_loss: 0.2445 - val_acc: 0.9048\n",
      "Epoch 17/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.2876 - acc: 0.8898 - val_loss: 0.2333 - val_acc: 0.9118\n",
      "Epoch 18/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.2724 - acc: 0.8948 - val_loss: 0.2216 - val_acc: 0.9146\n",
      "Epoch 19/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.2640 - acc: 0.8985 - val_loss: 0.2075 - val_acc: 0.9218\n",
      "Epoch 20/200\n",
      "44974/44974 [==============================] - 17s 386us/sample - loss: 0.2492 - acc: 0.9040 - val_loss: 0.1986 - val_acc: 0.9244\n",
      "Epoch 21/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.2388 - acc: 0.9090 - val_loss: 0.1948 - val_acc: 0.9256\n",
      "Epoch 22/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.2277 - acc: 0.9127 - val_loss: 0.1902 - val_acc: 0.9278\n",
      "Epoch 23/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.2193 - acc: 0.9166 - val_loss: 0.1828 - val_acc: 0.9326\n",
      "Epoch 24/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.2108 - acc: 0.9205 - val_loss: 0.1712 - val_acc: 0.9338\n",
      "Epoch 25/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.2021 - acc: 0.9245 - val_loss: 0.1693 - val_acc: 0.9356\n",
      "Epoch 26/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.1957 - acc: 0.9259 - val_loss: 0.1616 - val_acc: 0.9370\n",
      "Epoch 27/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.1875 - acc: 0.9291 - val_loss: 0.1546 - val_acc: 0.9400\n",
      "Epoch 28/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.1829 - acc: 0.9304 - val_loss: 0.1533 - val_acc: 0.9412\n",
      "Epoch 29/200\n",
      "44974/44974 [==============================] - 17s 375us/sample - loss: 0.1769 - acc: 0.9329 - val_loss: 0.1435 - val_acc: 0.9480\n",
      "Epoch 30/200\n",
      "44974/44974 [==============================] - 17s 385us/sample - loss: 0.1697 - acc: 0.9363 - val_loss: 0.1386 - val_acc: 0.9500\n",
      "Epoch 31/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.1678 - acc: 0.9364 - val_loss: 0.1355 - val_acc: 0.9512\n",
      "Epoch 32/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.1575 - acc: 0.9399 - val_loss: 0.1377 - val_acc: 0.9494\n",
      "Epoch 33/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.1522 - acc: 0.9426 - val_loss: 0.1302 - val_acc: 0.9536\n",
      "Epoch 34/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.1484 - acc: 0.9441 - val_loss: 0.1254 - val_acc: 0.9562\n",
      "Epoch 35/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.1459 - acc: 0.9446 - val_loss: 0.1296 - val_acc: 0.9568\n",
      "Epoch 36/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.1418 - acc: 0.9464 - val_loss: 0.1179 - val_acc: 0.9586\n",
      "Epoch 37/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.1372 - acc: 0.9474 - val_loss: 0.1198 - val_acc: 0.9574\n",
      "Epoch 38/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.1356 - acc: 0.9491 - val_loss: 0.1171 - val_acc: 0.9604\n",
      "Epoch 39/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.1306 - acc: 0.9508 - val_loss: 0.1133 - val_acc: 0.9620\n",
      "Epoch 40/200\n",
      "44974/44974 [==============================] - 17s 386us/sample - loss: 0.1272 - acc: 0.9534 - val_loss: 0.1107 - val_acc: 0.9624\n",
      "Epoch 41/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.1238 - acc: 0.9536 - val_loss: 0.1073 - val_acc: 0.9636\n",
      "Epoch 42/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.1217 - acc: 0.9547 - val_loss: 0.1060 - val_acc: 0.9636\n",
      "Epoch 43/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.1197 - acc: 0.9548 - val_loss: 0.1046 - val_acc: 0.9636\n",
      "Epoch 44/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.1130 - acc: 0.9569 - val_loss: 0.1026 - val_acc: 0.9640\n",
      "Epoch 45/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.1110 - acc: 0.9587 - val_loss: 0.0976 - val_acc: 0.9672\n",
      "Epoch 46/200\n",
      "44974/44974 [==============================] - 17s 375us/sample - loss: 0.1116 - acc: 0.9581 - val_loss: 0.0987 - val_acc: 0.9658\n",
      "Epoch 47/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.1081 - acc: 0.9589 - val_loss: 0.0964 - val_acc: 0.9652\n",
      "Epoch 48/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.1063 - acc: 0.9603 - val_loss: 0.0941 - val_acc: 0.9660\n",
      "Epoch 49/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.1030 - acc: 0.9619 - val_loss: 0.0947 - val_acc: 0.9662\n",
      "Epoch 50/200\n",
      "44974/44974 [==============================] - 17s 387us/sample - loss: 0.1009 - acc: 0.9617 - val_loss: 0.0955 - val_acc: 0.9652\n",
      "Epoch 51/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.0992 - acc: 0.9618 - val_loss: 0.0964 - val_acc: 0.9656\n",
      "Epoch 52/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0961 - acc: 0.9630 - val_loss: 0.0941 - val_acc: 0.9672\n",
      "Epoch 53/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0921 - acc: 0.9648 - val_loss: 0.0948 - val_acc: 0.9666\n",
      "Epoch 54/200\n",
      "44974/44974 [==============================] - 17s 375us/sample - loss: 0.0937 - acc: 0.9638 - val_loss: 0.0911 - val_acc: 0.9680\n",
      "Epoch 55/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0906 - acc: 0.9649 - val_loss: 0.0919 - val_acc: 0.9672\n",
      "Epoch 56/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.0911 - acc: 0.9650 - val_loss: 0.0914 - val_acc: 0.9682\n",
      "Epoch 57/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.0893 - acc: 0.9662 - val_loss: 0.0906 - val_acc: 0.9668\n",
      "Epoch 58/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0864 - acc: 0.9673 - val_loss: 0.0884 - val_acc: 0.9684\n",
      "Epoch 59/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0843 - acc: 0.9673 - val_loss: 0.0877 - val_acc: 0.9692\n",
      "Epoch 60/200\n",
      "44974/44974 [==============================] - 17s 386us/sample - loss: 0.0841 - acc: 0.9682 - val_loss: 0.0915 - val_acc: 0.9688\n",
      "Epoch 61/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.0813 - acc: 0.9690 - val_loss: 0.0897 - val_acc: 0.9688\n",
      "Epoch 62/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0839 - acc: 0.9678 - val_loss: 0.0881 - val_acc: 0.9696\n",
      "Epoch 63/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0785 - acc: 0.9702 - val_loss: 0.0900 - val_acc: 0.9688\n",
      "Epoch 64/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0795 - acc: 0.9685 - val_loss: 0.0855 - val_acc: 0.9704\n",
      "Epoch 65/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.0791 - acc: 0.9700 - val_loss: 0.0810 - val_acc: 0.9704\n",
      "Epoch 66/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0774 - acc: 0.9711 - val_loss: 0.0837 - val_acc: 0.9720\n",
      "Epoch 67/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.0758 - acc: 0.9706 - val_loss: 0.0894 - val_acc: 0.9700\n",
      "Epoch 68/200\n",
      "44974/44974 [==============================] - 17s 375us/sample - loss: 0.0745 - acc: 0.9710 - val_loss: 0.0876 - val_acc: 0.9710\n",
      "Epoch 69/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0730 - acc: 0.9724 - val_loss: 0.0893 - val_acc: 0.9696\n",
      "Epoch 70/200\n",
      "44974/44974 [==============================] - 17s 387us/sample - loss: 0.0742 - acc: 0.9715 - val_loss: 0.0839 - val_acc: 0.9706\n",
      "Epoch 71/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.0716 - acc: 0.9731 - val_loss: 0.0878 - val_acc: 0.9704\n",
      "Epoch 72/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0712 - acc: 0.9733 - val_loss: 0.0851 - val_acc: 0.9726\n",
      "Epoch 73/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0695 - acc: 0.9739 - val_loss: 0.0900 - val_acc: 0.9714\n",
      "Epoch 74/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0698 - acc: 0.9731 - val_loss: 0.0876 - val_acc: 0.9722\n",
      "Epoch 75/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.0676 - acc: 0.9740 - val_loss: 0.0868 - val_acc: 0.9716\n",
      "Epoch 76/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.0678 - acc: 0.9748 - val_loss: 0.0829 - val_acc: 0.9724\n",
      "Epoch 77/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0633 - acc: 0.9756 - val_loss: 0.0816 - val_acc: 0.9736\n",
      "Epoch 78/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0643 - acc: 0.9752 - val_loss: 0.0834 - val_acc: 0.9740\n",
      "Epoch 79/200\n",
      "44974/44974 [==============================] - 17s 375us/sample - loss: 0.0639 - acc: 0.9752 - val_loss: 0.0851 - val_acc: 0.9730\n",
      "Epoch 80/200\n",
      "44974/44974 [==============================] - 17s 387us/sample - loss: 0.0629 - acc: 0.9755 - val_loss: 0.0808 - val_acc: 0.9736\n",
      "Epoch 81/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0619 - acc: 0.9768 - val_loss: 0.0811 - val_acc: 0.9738\n",
      "Epoch 82/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.0630 - acc: 0.9767 - val_loss: 0.0785 - val_acc: 0.9750\n",
      "Epoch 83/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.0620 - acc: 0.9764 - val_loss: 0.0811 - val_acc: 0.9742\n",
      "Epoch 84/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0628 - acc: 0.9757 - val_loss: 0.0757 - val_acc: 0.9752\n",
      "Epoch 85/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0572 - acc: 0.9785 - val_loss: 0.0801 - val_acc: 0.9756\n",
      "Epoch 86/200\n",
      "44974/44974 [==============================] - 17s 375us/sample - loss: 0.0570 - acc: 0.9780 - val_loss: 0.0776 - val_acc: 0.9748\n",
      "Epoch 87/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0580 - acc: 0.9772 - val_loss: 0.0772 - val_acc: 0.9748\n",
      "Epoch 88/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0578 - acc: 0.9778 - val_loss: 0.0779 - val_acc: 0.9758\n",
      "Epoch 89/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0558 - acc: 0.9792 - val_loss: 0.0730 - val_acc: 0.9772\n",
      "Epoch 90/200\n",
      "44974/44974 [==============================] - 17s 388us/sample - loss: 0.0564 - acc: 0.9787 - val_loss: 0.0725 - val_acc: 0.9768\n",
      "Epoch 91/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0564 - acc: 0.9780 - val_loss: 0.0752 - val_acc: 0.9760\n",
      "Epoch 92/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0567 - acc: 0.9781 - val_loss: 0.0786 - val_acc: 0.9758\n",
      "Epoch 93/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0543 - acc: 0.9792 - val_loss: 0.0754 - val_acc: 0.9776\n",
      "Epoch 94/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0546 - acc: 0.9786 - val_loss: 0.0751 - val_acc: 0.9776\n",
      "Epoch 95/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.0544 - acc: 0.9789 - val_loss: 0.0757 - val_acc: 0.9774\n",
      "Epoch 96/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.0553 - acc: 0.9787 - val_loss: 0.0801 - val_acc: 0.9756\n",
      "Epoch 97/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0504 - acc: 0.9804 - val_loss: 0.0761 - val_acc: 0.9768\n",
      "Epoch 98/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0503 - acc: 0.9810 - val_loss: 0.0740 - val_acc: 0.9784\n",
      "Epoch 99/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0506 - acc: 0.9813 - val_loss: 0.0711 - val_acc: 0.9788\n",
      "Epoch 100/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0497 - acc: 0.9804 - val_loss: 0.0744 - val_acc: 0.9764\n",
      "Epoch 101/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0508 - acc: 0.9812 - val_loss: 0.0768 - val_acc: 0.9756\n",
      "Epoch 102/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0481 - acc: 0.9818 - val_loss: 0.0729 - val_acc: 0.9762\n",
      "Epoch 103/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0516 - acc: 0.9805 - val_loss: 0.0704 - val_acc: 0.9774\n",
      "Epoch 104/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0484 - acc: 0.9813 - val_loss: 0.0734 - val_acc: 0.9784\n",
      "Epoch 105/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0493 - acc: 0.9811 - val_loss: 0.0700 - val_acc: 0.9788\n",
      "Epoch 106/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0462 - acc: 0.9826 - val_loss: 0.0705 - val_acc: 0.9774\n",
      "Epoch 107/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0477 - acc: 0.9823 - val_loss: 0.0687 - val_acc: 0.9790\n",
      "Epoch 108/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0470 - acc: 0.9817 - val_loss: 0.0687 - val_acc: 0.9786\n",
      "Epoch 109/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0464 - acc: 0.9826 - val_loss: 0.0689 - val_acc: 0.9794\n",
      "Epoch 110/200\n",
      "44974/44974 [==============================] - 17s 387us/sample - loss: 0.0482 - acc: 0.9821 - val_loss: 0.0694 - val_acc: 0.9790\n",
      "Epoch 111/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0465 - acc: 0.9829 - val_loss: 0.0736 - val_acc: 0.9780\n",
      "Epoch 112/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0466 - acc: 0.9826 - val_loss: 0.0709 - val_acc: 0.9782\n",
      "Epoch 113/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0448 - acc: 0.9828 - val_loss: 0.0763 - val_acc: 0.9782\n",
      "Epoch 114/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0456 - acc: 0.9824 - val_loss: 0.0727 - val_acc: 0.9788\n",
      "Epoch 115/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.0430 - acc: 0.9838 - val_loss: 0.0696 - val_acc: 0.9794\n",
      "Epoch 116/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0428 - acc: 0.9839 - val_loss: 0.0708 - val_acc: 0.9792\n",
      "Epoch 117/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0418 - acc: 0.9849 - val_loss: 0.0694 - val_acc: 0.9798\n",
      "Epoch 118/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0413 - acc: 0.9842 - val_loss: 0.0715 - val_acc: 0.9790\n",
      "Epoch 119/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0426 - acc: 0.9839 - val_loss: 0.0699 - val_acc: 0.9814\n",
      "Epoch 120/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.0410 - acc: 0.9847 - val_loss: 0.0721 - val_acc: 0.9794\n",
      "Epoch 121/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0418 - acc: 0.9841 - val_loss: 0.0710 - val_acc: 0.9802\n",
      "Epoch 122/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0392 - acc: 0.9854 - val_loss: 0.0699 - val_acc: 0.9808\n",
      "Epoch 123/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0401 - acc: 0.9851 - val_loss: 0.0656 - val_acc: 0.9812\n",
      "Epoch 124/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0385 - acc: 0.9851 - val_loss: 0.0716 - val_acc: 0.9806\n",
      "Epoch 125/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.0390 - acc: 0.9858 - val_loss: 0.0721 - val_acc: 0.9812\n",
      "Epoch 126/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0392 - acc: 0.9855 - val_loss: 0.0694 - val_acc: 0.9808\n",
      "Epoch 127/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0384 - acc: 0.9850 - val_loss: 0.0715 - val_acc: 0.9804\n",
      "Epoch 128/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0378 - acc: 0.9857 - val_loss: 0.0714 - val_acc: 0.9804\n",
      "Epoch 129/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0380 - acc: 0.9859 - val_loss: 0.0744 - val_acc: 0.9794\n",
      "Epoch 130/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0384 - acc: 0.9856 - val_loss: 0.0758 - val_acc: 0.9798\n",
      "Epoch 131/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0386 - acc: 0.9855 - val_loss: 0.0732 - val_acc: 0.9790\n",
      "Epoch 132/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0387 - acc: 0.9852 - val_loss: 0.0725 - val_acc: 0.9800\n",
      "Epoch 133/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0352 - acc: 0.9864 - val_loss: 0.0757 - val_acc: 0.9798\n",
      "Epoch 134/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0385 - acc: 0.9853 - val_loss: 0.0748 - val_acc: 0.9800\n",
      "Epoch 135/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0360 - acc: 0.9867 - val_loss: 0.0765 - val_acc: 0.9804\n",
      "Epoch 136/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0369 - acc: 0.9860 - val_loss: 0.0705 - val_acc: 0.9820\n",
      "Epoch 137/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0337 - acc: 0.9870 - val_loss: 0.0824 - val_acc: 0.9810\n",
      "Epoch 138/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0356 - acc: 0.9865 - val_loss: 0.0814 - val_acc: 0.9794\n",
      "Epoch 139/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0368 - acc: 0.9864 - val_loss: 0.0770 - val_acc: 0.9812\n",
      "Epoch 140/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0377 - acc: 0.9861 - val_loss: 0.0761 - val_acc: 0.9816\n",
      "Epoch 141/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0346 - acc: 0.9865 - val_loss: 0.0724 - val_acc: 0.9820\n",
      "Epoch 142/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0349 - acc: 0.9869 - val_loss: 0.0734 - val_acc: 0.9806\n",
      "Epoch 143/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0354 - acc: 0.9867 - val_loss: 0.0707 - val_acc: 0.9824\n",
      "Epoch 144/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0322 - acc: 0.9882 - val_loss: 0.0736 - val_acc: 0.9810\n",
      "Epoch 145/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0353 - acc: 0.9869 - val_loss: 0.0748 - val_acc: 0.9806\n",
      "Epoch 146/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0331 - acc: 0.9877 - val_loss: 0.0718 - val_acc: 0.9808\n",
      "Epoch 147/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0332 - acc: 0.9880 - val_loss: 0.0742 - val_acc: 0.9804\n",
      "Epoch 148/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0318 - acc: 0.9878 - val_loss: 0.0708 - val_acc: 0.9812\n",
      "Epoch 149/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0316 - acc: 0.9880 - val_loss: 0.0743 - val_acc: 0.9820\n",
      "Epoch 150/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0301 - acc: 0.9886 - val_loss: 0.0700 - val_acc: 0.9814\n",
      "Epoch 151/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0336 - acc: 0.9874 - val_loss: 0.0729 - val_acc: 0.9812\n",
      "Epoch 152/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0314 - acc: 0.9882 - val_loss: 0.0738 - val_acc: 0.9818\n",
      "Epoch 153/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0316 - acc: 0.9882 - val_loss: 0.0766 - val_acc: 0.9818\n",
      "Epoch 154/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0331 - acc: 0.9874 - val_loss: 0.0733 - val_acc: 0.9820\n",
      "Epoch 155/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0312 - acc: 0.9887 - val_loss: 0.0743 - val_acc: 0.9806\n",
      "Epoch 156/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0321 - acc: 0.9881 - val_loss: 0.0698 - val_acc: 0.9816\n",
      "Epoch 157/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0308 - acc: 0.9887 - val_loss: 0.0732 - val_acc: 0.9814\n",
      "Epoch 158/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.0292 - acc: 0.9887 - val_loss: 0.0731 - val_acc: 0.9808\n",
      "Epoch 159/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0296 - acc: 0.9892 - val_loss: 0.0716 - val_acc: 0.9820\n",
      "Epoch 160/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0284 - acc: 0.9893 - val_loss: 0.0738 - val_acc: 0.9820\n",
      "Epoch 161/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0301 - acc: 0.9891 - val_loss: 0.0736 - val_acc: 0.9814\n",
      "Epoch 162/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0305 - acc: 0.9893 - val_loss: 0.0718 - val_acc: 0.9812\n",
      "Epoch 163/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0293 - acc: 0.9891 - val_loss: 0.0644 - val_acc: 0.9824\n",
      "Epoch 164/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0275 - acc: 0.9902 - val_loss: 0.0710 - val_acc: 0.9822\n",
      "Epoch 165/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0288 - acc: 0.9892 - val_loss: 0.0706 - val_acc: 0.9824\n",
      "Epoch 166/200\n",
      "44974/44974 [==============================] - 17s 370us/sample - loss: 0.0312 - acc: 0.9892 - val_loss: 0.0732 - val_acc: 0.9818\n",
      "Epoch 167/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0291 - acc: 0.9888 - val_loss: 0.0761 - val_acc: 0.9826\n",
      "Epoch 168/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0285 - acc: 0.9891 - val_loss: 0.0703 - val_acc: 0.9828\n",
      "Epoch 169/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0269 - acc: 0.9904 - val_loss: 0.0701 - val_acc: 0.9818\n",
      "Epoch 170/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0263 - acc: 0.9908 - val_loss: 0.0705 - val_acc: 0.9824\n",
      "Epoch 171/200\n",
      "44974/44974 [==============================] - 17s 370us/sample - loss: 0.0273 - acc: 0.9901 - val_loss: 0.0708 - val_acc: 0.9828\n",
      "Epoch 172/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0259 - acc: 0.9906 - val_loss: 0.0733 - val_acc: 0.9832\n",
      "Epoch 173/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0252 - acc: 0.9907 - val_loss: 0.0685 - val_acc: 0.9834\n",
      "Epoch 174/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0277 - acc: 0.9899 - val_loss: 0.0692 - val_acc: 0.9834\n",
      "Epoch 175/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0263 - acc: 0.9904 - val_loss: 0.0695 - val_acc: 0.9826\n",
      "Epoch 176/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.0280 - acc: 0.9902 - val_loss: 0.0678 - val_acc: 0.9832\n",
      "Epoch 177/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.0249 - acc: 0.9910 - val_loss: 0.0685 - val_acc: 0.9838\n",
      "Epoch 178/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0260 - acc: 0.9908 - val_loss: 0.0688 - val_acc: 0.9838\n",
      "Epoch 179/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0278 - acc: 0.9897 - val_loss: 0.0733 - val_acc: 0.9830\n",
      "Epoch 180/200\n",
      "44974/44974 [==============================] - 17s 387us/sample - loss: 0.0240 - acc: 0.9913 - val_loss: 0.0693 - val_acc: 0.9840\n",
      "Epoch 181/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0270 - acc: 0.9903 - val_loss: 0.0748 - val_acc: 0.9834\n",
      "Epoch 182/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0252 - acc: 0.9907 - val_loss: 0.0739 - val_acc: 0.9824\n",
      "Epoch 183/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0253 - acc: 0.9905 - val_loss: 0.0679 - val_acc: 0.9824\n",
      "Epoch 184/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0253 - acc: 0.9909 - val_loss: 0.0695 - val_acc: 0.9834\n",
      "Epoch 185/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0265 - acc: 0.9903 - val_loss: 0.0638 - val_acc: 0.9842\n",
      "Epoch 186/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0247 - acc: 0.9912 - val_loss: 0.0696 - val_acc: 0.9842\n",
      "Epoch 187/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0257 - acc: 0.9908 - val_loss: 0.0659 - val_acc: 0.9844\n",
      "Epoch 188/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0241 - acc: 0.9914 - val_loss: 0.0706 - val_acc: 0.9840\n",
      "Epoch 189/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0238 - acc: 0.9915 - val_loss: 0.0717 - val_acc: 0.9834\n",
      "Epoch 190/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0232 - acc: 0.9924 - val_loss: 0.0703 - val_acc: 0.9838\n",
      "Epoch 191/200\n",
      "44974/44974 [==============================] - 17s 374us/sample - loss: 0.0259 - acc: 0.9905 - val_loss: 0.0714 - val_acc: 0.9834\n",
      "Epoch 192/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0242 - acc: 0.9912 - val_loss: 0.0656 - val_acc: 0.9844\n",
      "Epoch 193/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0239 - acc: 0.9915 - val_loss: 0.0695 - val_acc: 0.9846\n",
      "Epoch 194/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0222 - acc: 0.9922 - val_loss: 0.0741 - val_acc: 0.9846\n",
      "Epoch 195/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0240 - acc: 0.9920 - val_loss: 0.0678 - val_acc: 0.9854\n",
      "Epoch 196/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0216 - acc: 0.9920 - val_loss: 0.0689 - val_acc: 0.9850\n",
      "Epoch 197/200\n",
      "44974/44974 [==============================] - 17s 372us/sample - loss: 0.0222 - acc: 0.9918 - val_loss: 0.0712 - val_acc: 0.9846\n",
      "Epoch 198/200\n",
      "44974/44974 [==============================] - 17s 373us/sample - loss: 0.0225 - acc: 0.9928 - val_loss: 0.0695 - val_acc: 0.9848\n",
      "Epoch 199/200\n",
      "44974/44974 [==============================] - 17s 371us/sample - loss: 0.0221 - acc: 0.9921 - val_loss: 0.0674 - val_acc: 0.9850\n",
      "Epoch 200/200\n",
      "44974/44974 [==============================] - 17s 386us/sample - loss: 0.0226 - acc: 0.9923 - val_loss: 0.0685 - val_acc: 0.9848\n"
     ]
    }
   ],
   "source": [
    "model = trainModel(builtCompositeModel(drop_out_rate=0.5),[train_headline, train_first_body, train_last_body],[vali_headline, vali_first_body, vali_last_body])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 91893,
     "status": "ok",
     "timestamp": 1563908700752,
     "user": {
      "displayName": "chuck fu",
      "photoUrl": "",
      "userId": "01740202220584032426"
     },
     "user_tz": 240
    },
    "id": "Tz-8640eL3xE",
    "outputId": "f87b7cb0-a063-4db3-ba65-cc8cba9bdf45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores on the test set\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    342    |     8     |     8     |     4     |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |     9     |    69     |     0     |     1     |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |     7     |     1     |    886    |     4     |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |     9     |     2     |    23     |   3625    |\n",
      "-------------------------------------------------------------\n",
      "Score: 2211.5 out of 2253.75\t(98.12534664448143%)\n",
      "Scores on the test set\n",
      "-------------------------------------------------------------\n",
      "|           |   agree   | disagree  |  discuss  | unrelated |\n",
      "-------------------------------------------------------------\n",
      "|   agree   |    847    |    35     |    333    |    688    |\n",
      "-------------------------------------------------------------\n",
      "| disagree  |    210    |    36     |    110    |    341    |\n",
      "-------------------------------------------------------------\n",
      "|  discuss  |    980    |    121    |   2163    |   1200    |\n",
      "-------------------------------------------------------------\n",
      "| unrelated |   1043    |    56     |    815    |   16435   |\n",
      "-------------------------------------------------------------\n",
      "Score: 7602.0 out of 11651.25\t(65.24621821692952%)\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict([vali_headline, vali_first_body,vali_last_body])\n",
    "y_classes = pred.argmax(axis=-1)\n",
    "actual = y_vali\n",
    "evaluate(y_classes, actual)\n",
    "\n",
    "\n",
    "\n",
    "pred = model.predict([competition_headline, competition_first_body, competition_last_body])\n",
    "y_classes = pred.argmax(axis=-1)\n",
    "actual = compe['Stance']\n",
    "evaluate(y_classes, actual)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "try.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
